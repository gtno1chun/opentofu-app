{"version":4,"terraform_version":"1.8.0","serial":11,"lineage":"2687f70b-ea02-82eb-37d4-0c924485cdf3","outputs":{"k8s_context":{"value":"was-admin@cluster","type":"string"}},"resources":[{"mode":"managed","type":"helm_release","name":"ingress-nginx","provider":"provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"schema_version":1,"attributes":{"atomic":false,"chart":"ingress-nginx","cleanup_on_fail":false,"create_namespace":false,"dependency_update":false,"description":null,"devel":null,"disable_crd_hooks":false,"disable_openapi_validation":false,"disable_webhooks":false,"force_update":false,"id":"ingress-nginx","keyring":null,"lint":false,"manifest":null,"max_history":0,"metadata":[{"app_version":"1.10.3","chart":"ingress-nginx","first_deployed":1722410181,"last_deployed":1722410181,"name":"ingress-nginx","namespace":"ingress-nginx","notes":"The ingress-nginx controller has been installed.\nGet the application URL by running these commands:\n  export HTTP_NODE_PORT=$(kubectl get service --namespace ingress-nginx ingress-nginx-controller --output jsonpath=\"{.spec.ports[0].nodePort}\")\n  export HTTPS_NODE_PORT=$(kubectl get service --namespace ingress-nginx ingress-nginx-controller --output jsonpath=\"{.spec.ports[1].nodePort}\")\n  export NODE_IP=\"$(kubectl get nodes --output jsonpath=\"{.items[0].status.addresses[1].address}\")\"\n\n  echo \"Visit http://${NODE_IP}:${HTTP_NODE_PORT} to access your application via HTTP.\"\n  echo \"Visit https://${NODE_IP}:${HTTPS_NODE_PORT} to access your application via HTTPS.\"\n\nAn example Ingress that makes use of the controller:\n  apiVersion: networking.k8s.io/v1\n  kind: Ingress\n  metadata:\n    name: example\n    namespace: foo\n  spec:\n    ingressClassName: nginx\n    rules:\n      - host: www.example.com\n        http:\n          paths:\n            - pathType: Prefix\n              backend:\n                service:\n                  name: exampleService\n                  port:\n                    number: 80\n              path: /\n    # This section is only required if TLS is to be enabled for the Ingress\n    tls:\n      - hosts:\n        - www.example.com\n        secretName: example-tls\n\nIf TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n\n  apiVersion: v1\n  kind: Secret\n  metadata:\n    name: example-tls\n    namespace: foo\n  data:\n    tls.crt: \u003cbase64 encoded cert\u003e\n    tls.key: \u003cbase64 encoded key\u003e\n  type: kubernetes.io/tls\n","revision":1,"values":"{\"commonLabels\":{},\"controller\":{\"addHeaders\":{},\"admissionWebhooks\":{\"annotations\":{},\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"certificate\":\"/usr/local/certificates/cert\",\"createSecretJob\":{\"name\":\"create\",\"resources\":{},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":65532,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"enabled\":true,\"existingPsp\":\"\",\"extraEnvs\":[],\"failurePolicy\":\"Ignore\",\"key\":\"/usr/local/certificates/key\",\"labels\":{},\"name\":\"admission\",\"namespaceSelector\":{},\"objectSelector\":{},\"patch\":{\"enabled\":true,\"image\":{\"digest\":\"sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\",\"image\":\"ingress-nginx/kube-webhook-certgen\",\"pullPolicy\":\"IfNotPresent\",\"registry\":\"registry.k8s.io\",\"tag\":\"v1.4.1\"},\"labels\":{},\"networkPolicy\":{\"enabled\":false},\"nodeSelector\":{\"kubernetes.io/os\":\"linux\"},\"podAnnotations\":{},\"priorityClassName\":\"\",\"securityContext\":{},\"tolerations\":[]},\"patchWebhookJob\":{\"name\":\"patch\",\"resources\":{},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":65532,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"port\":8443,\"service\":{\"annotations\":{},\"externalIPs\":[],\"loadBalancerSourceRanges\":[],\"servicePort\":443,\"type\":\"ClusterIP\"}},\"affinity\":{},\"allowSnippetAnnotations\":false,\"annotations\":{},\"autoscaling\":{\"annotations\":{},\"behavior\":{},\"enabled\":false,\"maxReplicas\":11,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"autoscalingTemplate\":[],\"config\":{},\"configAnnotations\":{},\"configMapNamespace\":\"\",\"containerName\":\"controller\",\"containerPort\":{\"http\":80,\"https\":443},\"containerSecurityContext\":{},\"customTemplate\":{\"configMapKey\":\"\",\"configMapName\":\"\"},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"electionID\":\"\",\"enableAnnotationValidations\":false,\"enableMimalloc\":true,\"enableTopologyAwareRouting\":false,\"existingPsp\":\"\",\"extraArgs\":{},\"extraContainers\":[],\"extraEnvs\":[],\"extraInitContainers\":[],\"extraModules\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"healthCheckHost\":\"\",\"healthCheckPath\":\"/healthz\",\"hostAliases\":[],\"hostNetwork\":false,\"hostPort\":{\"enabled\":false,\"ports\":{\"http\":80,\"https\":443}},\"hostname\":{},\"image\":{\"allowPrivilegeEscalation\":false,\"chroot\":false,\"digest\":\"sha256:b5a5082f8e508cc1aac1c0ef101dc2f87b63d51598a5747d81d6cf6e7ba058fd\",\"digestChroot\":\"sha256:9033e04bd3cd01f92414f8d5999c5095734d4caceb4923942298152a38373d4b\",\"image\":\"ingress-nginx/controller\",\"pullPolicy\":\"IfNotPresent\",\"readOnlyRootFilesystem\":false,\"registry\":\"registry.k8s.io\",\"runAsNonRoot\":true,\"runAsUser\":101,\"seccompProfile\":{\"type\":\"RuntimeDefault\"},\"tag\":\"v1.10.3\"},\"ingressClass\":\"nginx\",\"ingressClassByName\":false,\"ingressClassResource\":{\"controllerValue\":\"k8s.io/ingress-nginx\",\"default\":false,\"enabled\":true,\"name\":\"nginx\",\"parameters\":{}},\"keda\":{\"apiVersion\":\"keda.sh/v1alpha1\",\"behavior\":{},\"cooldownPeriod\":300,\"enabled\":false,\"maxReplicas\":11,\"minReplicas\":1,\"pollingInterval\":30,\"restoreToOriginalReplicaCount\":false,\"scaledObject\":{\"annotations\":{}},\"triggers\":[]},\"kind\":\"Deployment\",\"labels\":{},\"lifecycle\":{\"preStop\":{\"exec\":{\"command\":[\"/wait-shutdown\"]}}},\"livenessProbe\":{\"failureThreshold\":5,\"httpGet\":{\"path\":\"/healthz\",\"port\":10254,\"scheme\":\"HTTP\"},\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"maxmindLicenseKey\":\"\",\"metrics\":{\"enabled\":false,\"port\":10254,\"portName\":\"metrics\",\"prometheusRule\":{\"additionalLabels\":{},\"enabled\":false,\"rules\":[]},\"service\":{\"annotations\":{},\"externalIPs\":[],\"labels\":{},\"loadBalancerSourceRanges\":[],\"servicePort\":10254,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"metricRelabelings\":[],\"namespace\":\"\",\"namespaceSelector\":{},\"relabelings\":[],\"scrapeInterval\":\"30s\",\"targetLabels\":[]}},\"minAvailable\":1,\"minReadySeconds\":0,\"name\":\"controller\",\"networkPolicy\":{\"enabled\":false},\"nodeSelector\":{\"kubernetes.io/os\":\"linux\"},\"opentelemetry\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":65532,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"enabled\":false,\"image\":{\"digest\":\"sha256:13bee3f5223883d3ca62fee7309ad02d22ec00ff0d7033e3e9aca7a9f60fd472\",\"distroless\":true,\"image\":\"ingress-nginx/opentelemetry\",\"registry\":\"registry.k8s.io\",\"tag\":\"v20230721-3e2062ee5\"},\"name\":\"opentelemetry\",\"resources\":{}},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{},\"priorityClassName\":\"\",\"proxySetHeaders\":{},\"publishService\":{\"enabled\":true,\"pathOverride\":\"\"},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/healthz\",\"port\":10254,\"scheme\":\"HTTP\"},\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicaCount\":1,\"reportNodeInternalIp\":false,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"90Mi\"}},\"scope\":{\"enabled\":false,\"namespace\":\"\",\"namespaceSelector\":\"\"},\"service\":{\"annotations\":{},\"appProtocol\":true,\"clusterIP\":\"\",\"enableHttp\":true,\"enableHttps\":true,\"enabled\":true,\"external\":{\"enabled\":true},\"externalIPs\":[],\"externalTrafficPolicy\":\"\",\"internal\":{\"annotations\":{},\"appProtocol\":true,\"clusterIP\":\"\",\"enabled\":false,\"externalIPs\":[],\"externalTrafficPolicy\":\"\",\"ipFamilies\":[\"IPv4\"],\"ipFamilyPolicy\":\"SingleStack\",\"loadBalancerClass\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\",\"https\":\"\",\"tcp\":{},\"udp\":{}},\"ports\":{},\"sessionAffinity\":\"\",\"targetPorts\":{},\"type\":\"\"},\"ipFamilies\":[\"IPv4\"],\"ipFamilyPolicy\":\"SingleStack\",\"labels\":{},\"loadBalancerClass\":\"\",\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"http\":\"\",\"https\":\"\",\"tcp\":{},\"udp\":{}},\"ports\":{\"http\":80,\"https\":443},\"sessionAffinity\":\"\",\"targetPorts\":{\"http\":\"http\",\"https\":\"https\"},\"type\":\"NodePort\"},\"shareProcessNamespace\":false,\"sysctls\":{},\"tcp\":{\"annotations\":{},\"configMapNamespace\":\"\"},\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[],\"udp\":{\"annotations\":{},\"configMapNamespace\":\"\"},\"updateStrategy\":{},\"watchIngressWithoutClass\":false},\"defaultBackend\":{\"affinity\":{},\"autoscaling\":{\"annotations\":{},\"enabled\":false,\"maxReplicas\":2,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"containerSecurityContext\":{},\"enabled\":false,\"existingPsp\":\"\",\"extraArgs\":{},\"extraConfigMaps\":[],\"extraEnvs\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"allowPrivilegeEscalation\":false,\"image\":\"defaultbackend-amd64\",\"pullPolicy\":\"IfNotPresent\",\"readOnlyRootFilesystem\":true,\"registry\":\"registry.k8s.io\",\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"},\"tag\":\"1.5\"},\"labels\":{},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"minAvailable\":1,\"minReadySeconds\":0,\"name\":\"defaultbackend\",\"networkPolicy\":{\"enabled\":false},\"nodeSelector\":{\"kubernetes.io/os\":\"linux\"},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{},\"port\":8080,\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":6,\"initialDelaySeconds\":0,\"periodSeconds\":5,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":1,\"resources\":{},\"service\":{\"annotations\":{},\"externalIPs\":[],\"loadBalancerSourceRanges\":[],\"servicePort\":80,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"tolerations\":[],\"updateStrategy\":{}},\"dhParam\":\"\",\"imagePullSecrets\":[],\"namespaceOverride\":\"\",\"podSecurityPolicy\":{\"enabled\":false},\"portNamePrefix\":\"\",\"rbac\":{\"create\":true,\"scope\":false},\"revisionHistoryLimit\":10,\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"tcp\":{},\"udp\":{}}","version":"4.10.3"}],"name":"ingress-nginx","namespace":"ingress-nginx","pass_credentials":false,"postrender":[],"recreate_pods":false,"render_subchart_notes":true,"replace":false,"repository":"./helm","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":[{"name":"controller.admissionWebhooks.failurePolicy","type":"","value":"Ignore"},{"name":"controller.service.type","type":"","value":"NodePort"}],"set_list":[],"set_sensitive":[],"skip_crds":false,"status":"deployed","timeout":300,"values":["## nginx configuration\n## Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/index.md\n##\n\n## Overrides for generated resource names\n# See templates/_helpers.tpl\n# nameOverride:\n# fullnameOverride:\n\n# -- Override the deployment namespace; defaults to .Release.Namespace\nnamespaceOverride: \"\"\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\ncontroller:\n  name: controller\n  enableAnnotationValidations: false\n  image:\n    ## Keep false as default for now!\n    chroot: false\n    registry: registry.k8s.io\n    image: ingress-nginx/controller\n    ## for backwards compatibility consider setting the full image url via the repository value below\n    ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail\n    ## repository:\n    tag: \"v1.10.3\"\n    digest: sha256:b5a5082f8e508cc1aac1c0ef101dc2f87b63d51598a5747d81d6cf6e7ba058fd\n    digestChroot: sha256:9033e04bd3cd01f92414f8d5999c5095734d4caceb4923942298152a38373d4b\n    pullPolicy: IfNotPresent\n    runAsNonRoot: true\n    # www-data -\u003e uid 101\n    runAsUser: 101\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    readOnlyRootFilesystem: false\n  # -- Use an existing PSP instead of creating one\n  existingPsp: \"\"\n  # -- Configures the controller container name\n  containerName: controller\n  # -- Configures the ports that the nginx-controller listens on\n  containerPort:\n    http: 80\n    https: 443\n  # -- Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/\n  config: {}\n  # -- Annotations to be added to the controller config configuration configmap.\n  configAnnotations: {}\n  # -- Will add custom headers before sending traffic to backends according to https://github.com/kubernetes/ingress-nginx/tree/main/docs/examples/customization/custom-headers\n  proxySetHeaders: {}\n  # -- Will add custom headers before sending response traffic to the client according to: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#add-headers\n  addHeaders: {}\n  # -- Optionally customize the pod dnsConfig.\n  dnsConfig: {}\n  # -- Optionally customize the pod hostAliases.\n  hostAliases: []\n  # - ip: 127.0.0.1\n  #   hostnames:\n  #   - foo.local\n  #   - bar.local\n  # - ip: 10.1.2.3\n  #   hostnames:\n  #   - foo.remote\n  #   - bar.remote\n  # -- Optionally customize the pod hostname.\n  hostname: {}\n  # -- Optionally change this to ClusterFirstWithHostNet in case you have 'hostNetwork: true'.\n  # By default, while using host network, name resolution uses the host's DNS. If you wish nginx-controller\n  # to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.\n  dnsPolicy: ClusterFirst\n  # -- Bare-metal considerations via the host network https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#via-the-host-network\n  # Ingress status was blank because there is no Service exposing the Ingress-Nginx Controller in a configuration using the host network, the default --publish-service flag used in standard cloud setups does not apply\n  reportNodeInternalIp: false\n  # -- Process Ingress objects without ingressClass annotation/ingressClassName field\n  # Overrides value for --watch-ingress-without-class flag of the controller binary\n  # Defaults to false\n  watchIngressWithoutClass: false\n  # -- Process IngressClass per name (additionally as per spec.controller).\n  ingressClassByName: false\n  # -- This configuration enables Topology Aware Routing feature, used together with service annotation service.kubernetes.io/topology-mode=\"auto\"\n  # Defaults to false\n  enableTopologyAwareRouting: false\n  # -- This configuration defines if Ingress Controller should allow users to set\n  # their own *-snippet annotations, otherwise this is forbidden / dropped\n  # when users add those annotations.\n  # Global snippets in ConfigMap are still respected\n  allowSnippetAnnotations: false\n  # -- Required for use with CNI based kubernetes installations (such as ones set up by kubeadm),\n  # since CNI and hostport don't mix yet. Can be deprecated once https://github.com/kubernetes/kubernetes/issues/23920\n  # is merged\n  hostNetwork: false\n  ## Use host ports 80 and 443\n  ## Disabled by default\n  hostPort:\n    # -- Enable 'hostPort' or not\n    enabled: false\n    ports:\n      # -- 'hostPort' http port\n      http: 80\n      # -- 'hostPort' https port\n      https: 443\n  # NetworkPolicy for controller component.\n  networkPolicy:\n    # -- Enable 'networkPolicy' or not\n    enabled: false\n  # -- Election ID to use for status update, by default it uses the controller name combined with a suffix of 'leader'\n  electionID: \"\"\n  # -- This section refers to the creation of the IngressClass resource.\n  # IngressClasses are immutable and cannot be changed after creation.\n  # We do not support namespaced IngressClasses, yet, so a ClusterRole and a ClusterRoleBinding is required.\n  ingressClassResource:\n    # -- Name of the IngressClass\n    name: nginx\n    # -- Create the IngressClass or not\n    enabled: true\n    # -- If true, Ingresses without `ingressClassName` get assigned to this IngressClass on creation.\n    # Ingress creation gets rejected if there are multiple default IngressClasses.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#default-ingress-class\n    default: false\n    # -- Controller of the IngressClass. An Ingress Controller looks for IngressClasses it should reconcile by this value.\n    # This value is also being set as the `--controller-class` argument of this Ingress Controller.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class\n    controllerValue: k8s.io/ingress-nginx\n    # -- A link to a custom resource containing additional configuration for the controller.\n    # This is optional if the controller consuming this IngressClass does not require additional parameters.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class\n    parameters: {}\n    # parameters:\n    #   apiGroup: k8s.example.com\n    #   kind: IngressParameters\n    #   name: external-lb\n  # -- For backwards compatibility with ingress.class annotation, use ingressClass.\n  # Algorithm is as follows, first ingressClassName is considered, if not present, controller looks for ingress.class annotation\n  ingressClass: nginx\n  # -- Labels to add to the pod container metadata\n  podLabels: {}\n  #  key: value\n\n  # -- Security context for controller pods\n  podSecurityContext: {}\n  # -- sysctls for controller pods\n  ## Ref: https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/\n  sysctls: {}\n  # sysctls:\n  #   \"net.core.somaxconn\": \"8192\"\n  # -- Security context for controller containers\n  containerSecurityContext: {}\n  # -- Allows customization of the source of the IP address or FQDN to report\n  # in the ingress status field. By default, it reads the information provided\n  # by the service. If disable, the status field reports the IP address of the\n  # node or nodes where an ingress controller pod is running.\n  publishService:\n    # -- Enable 'publishService' or not\n    enabled: true\n    # -- Allows overriding of the publish service to bind to\n    # Must be \u003cnamespace\u003e/\u003cservice_name\u003e\n    pathOverride: \"\"\n  # Limit the scope of the controller to a specific namespace\n  scope:\n    # -- Enable 'scope' or not\n    enabled: false\n    # -- Namespace to limit the controller to; defaults to $(POD_NAMESPACE)\n    namespace: \"\"\n    # -- When scope.enabled == false, instead of watching all namespaces, we watching namespaces whose labels\n    # only match with namespaceSelector. Format like foo=bar. Defaults to empty, means watching all namespaces.\n    namespaceSelector: \"\"\n  # -- Allows customization of the configmap / nginx-configmap namespace; defaults to $(POD_NAMESPACE)\n  configMapNamespace: \"\"\n  tcp:\n    # -- Allows customization of the tcp-services-configmap; defaults to $(POD_NAMESPACE)\n    configMapNamespace: \"\"\n    # -- Annotations to be added to the tcp config configmap\n    annotations: {}\n  udp:\n    # -- Allows customization of the udp-services-configmap; defaults to $(POD_NAMESPACE)\n    configMapNamespace: \"\"\n    # -- Annotations to be added to the udp config configmap\n    annotations: {}\n  # -- Maxmind license key to download GeoLite2 Databases.\n  ## https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases\n  maxmindLicenseKey: \"\"\n  # -- Additional command line arguments to pass to Ingress-Nginx Controller\n  # E.g. to specify the default SSL certificate you can use\n  extraArgs: {}\n  ## extraArgs:\n  ##   default-ssl-certificate: \"\u003cnamespace\u003e/\u003csecret_name\u003e\"\n  ##   time-buckets: \"0.005,0.01,0.025,0.05,0.1,0.25,0.5,1,2.5,5,10\"\n  ##   length-buckets: \"10,20,30,40,50,60,70,80,90,100\"\n  ##   size-buckets: \"10,100,1000,10000,100000,1e+06,1e+07\"\n\n  # -- Additional environment variables to set\n  extraEnvs: []\n  # extraEnvs:\n  #   - name: FOO\n  #     valueFrom:\n  #       secretKeyRef:\n  #         key: FOO\n  #         name: secret-resource\n\n  # -- Use a `DaemonSet` or `Deployment`\n  kind: Deployment\n  # -- Annotations to be added to the controller Deployment or DaemonSet\n  ##\n  annotations: {}\n  #  keel.sh/pollSchedule: \"@every 60m\"\n\n  # -- Labels to be added to the controller Deployment or DaemonSet and other resources that do not have option to specify labels\n  ##\n  labels: {}\n  #  keel.sh/policy: patch\n  #  keel.sh/trigger: poll\n\n  # -- The update strategy to apply to the Deployment or DaemonSet\n  ##\n  updateStrategy: {}\n  #  rollingUpdate:\n  #    maxUnavailable: 1\n  #  type: RollingUpdate\n\n  # -- `minReadySeconds` to avoid killing pods before we are ready\n  ##\n  minReadySeconds: 0\n  # -- Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n  #  - key: \"key\"\n  #    operator: \"Equal|Exists\"\n  #    value: \"value\"\n  #    effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  # -- Affinity and anti-affinity rules for server scheduling to nodes\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ##\n  affinity: {}\n  # # An example of preferred pod anti-affinity, weight is in the range 1-100\n  # podAntiAffinity:\n  #   preferredDuringSchedulingIgnoredDuringExecution:\n  #   - weight: 100\n  #     podAffinityTerm:\n  #       labelSelector:\n  #         matchExpressions:\n  #         - key: app.kubernetes.io/name\n  #           operator: In\n  #           values:\n  #           - ingress-nginx\n  #         - key: app.kubernetes.io/instance\n  #           operator: In\n  #           values:\n  #           - ingress-nginx\n  #         - key: app.kubernetes.io/component\n  #           operator: In\n  #           values:\n  #           - controller\n  #       topologyKey: kubernetes.io/hostname\n\n  # # An example of required pod anti-affinity\n  # podAntiAffinity:\n  #   requiredDuringSchedulingIgnoredDuringExecution:\n  #   - labelSelector:\n  #       matchExpressions:\n  #       - key: app.kubernetes.io/name\n  #         operator: In\n  #         values:\n  #         - ingress-nginx\n  #       - key: app.kubernetes.io/instance\n  #         operator: In\n  #         values:\n  #         - ingress-nginx\n  #       - key: app.kubernetes.io/component\n  #         operator: In\n  #         values:\n  #         - controller\n  #     topologyKey: \"kubernetes.io/hostname\"\n\n  # -- Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ##\n  topologySpreadConstraints: []\n  # - labelSelector:\n  #     matchLabels:\n  #       app.kubernetes.io/name: '{{ include \"ingress-nginx.name\" . }}'\n  #       app.kubernetes.io/instance: '{{ .Release.Name }}'\n  #       app.kubernetes.io/component: controller\n  #   topologyKey: topology.kubernetes.io/zone\n  #   maxSkew: 1\n  #   whenUnsatisfiable: ScheduleAnyway\n  # - labelSelector:\n  #     matchLabels:\n  #       app.kubernetes.io/name: '{{ include \"ingress-nginx.name\" . }}'\n  #       app.kubernetes.io/instance: '{{ .Release.Name }}'\n  #       app.kubernetes.io/component: controller\n  #   topologyKey: kubernetes.io/hostname\n  #   maxSkew: 1\n  #   whenUnsatisfiable: ScheduleAnyway\n\n  # -- `terminationGracePeriodSeconds` to avoid killing pods before we are ready\n  ## wait up to five minutes for the drain of connections\n  ##\n  terminationGracePeriodSeconds: 300\n  # -- Node labels for controller pod assignment\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\n  ##\n  nodeSelector:\n    kubernetes.io/os: linux\n  ## Liveness and readiness probe values\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n  ##\n  ## startupProbe:\n  ##   httpGet:\n  ##     # should match container.healthCheckPath\n  ##     path: \"/healthz\"\n  ##     port: 10254\n  ##     scheme: HTTP\n  ##   initialDelaySeconds: 5\n  ##   periodSeconds: 5\n  ##   timeoutSeconds: 2\n  ##   successThreshold: 1\n  ##   failureThreshold: 5\n  livenessProbe:\n    httpGet:\n      # should match container.healthCheckPath\n      path: \"/healthz\"\n      port: 10254\n      scheme: HTTP\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 5\n  readinessProbe:\n    httpGet:\n      # should match container.healthCheckPath\n      path: \"/healthz\"\n      port: 10254\n      scheme: HTTP\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    successThreshold: 1\n    failureThreshold: 3\n  # -- Path of the health check endpoint. All requests received on the port defined by\n  # the healthz-port parameter are forwarded internally to this path.\n  healthCheckPath: \"/healthz\"\n  # -- Address to bind the health check endpoint.\n  # It is better to set this option to the internal node address\n  # if the Ingress-Nginx Controller is running in the `hostNetwork: true` mode.\n  healthCheckHost: \"\"\n  # -- Annotations to be added to controller pods\n  ##\n  podAnnotations: {}\n  replicaCount: 1\n  # -- Minimum available pods set in PodDisruptionBudget.\n  # Define either 'minAvailable' or 'maxUnavailable', never both.\n  minAvailable: 1\n  # -- Maximum unavailable pods set in PodDisruptionBudget. If set, 'minAvailable' is ignored.\n  # maxUnavailable: 1\n\n  ## Define requests resources to avoid probe issues due to CPU utilization in busy nodes\n  ## ref: https://github.com/kubernetes/ingress-nginx/issues/4735#issuecomment-551204903\n  ## Ideally, there should be no limits.\n  ## https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/\n  resources:\n    ##  limits:\n    ##    cpu: 100m\n    ##    memory: 90Mi\n    requests:\n      cpu: 100m\n      memory: 90Mi\n  # Mutually exclusive with keda autoscaling\n  autoscaling:\n    enabled: false\n    annotations: {}\n    minReplicas: 1\n    maxReplicas: 11\n    targetCPUUtilizationPercentage: 50\n    targetMemoryUtilizationPercentage: 50\n    behavior: {}\n    # scaleDown:\n    #   stabilizationWindowSeconds: 300\n    #   policies:\n    #   - type: Pods\n    #     value: 1\n    #     periodSeconds: 180\n    # scaleUp:\n    #   stabilizationWindowSeconds: 300\n    #   policies:\n    #   - type: Pods\n    #     value: 2\n    #     periodSeconds: 60\n  autoscalingTemplate: []\n  # Custom or additional autoscaling metrics\n  # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics\n  # - type: Pods\n  #   pods:\n  #     metric:\n  #       name: nginx_ingress_controller_nginx_process_requests_total\n  #     target:\n  #       type: AverageValue\n  #       averageValue: 10000m\n\n  # Mutually exclusive with hpa autoscaling\n  keda:\n    apiVersion: \"keda.sh/v1alpha1\"\n    ## apiVersion changes with keda 1.x vs 2.x\n    ## 2.x = keda.sh/v1alpha1\n    ## 1.x = keda.k8s.io/v1alpha1\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 11\n    pollingInterval: 30\n    cooldownPeriod: 300\n    # fallback:\n    #   failureThreshold: 3\n    #   replicas: 11\n    restoreToOriginalReplicaCount: false\n    scaledObject:\n      annotations: {}\n      # Custom annotations for ScaledObject resource\n      #  annotations:\n      # key: value\n    triggers: []\n    # - type: prometheus\n    #   metadata:\n    #     serverAddress: http://\u003cprometheus-host\u003e:9090\n    #     metricName: http_requests_total\n    #     threshold: '100'\n    #     query: sum(rate(http_requests_total{deployment=\"my-deployment\"}[2m]))\n\n    behavior: {}\n    # scaleDown:\n    #   stabilizationWindowSeconds: 300\n    #   policies:\n    #   - type: Pods\n    #     value: 1\n    #     periodSeconds: 180\n    # scaleUp:\n    #   stabilizationWindowSeconds: 300\n    #   policies:\n    #   - type: Pods\n    #     value: 2\n    #     periodSeconds: 60\n  # -- Enable mimalloc as a drop-in replacement for malloc.\n  ## ref: https://github.com/microsoft/mimalloc\n  ##\n  enableMimalloc: true\n  ## Override NGINX template\n  customTemplate:\n    configMapName: \"\"\n    configMapKey: \"\"\n  service:\n    # -- Enable controller services or not. This does not influence the creation of either the admission webhook or the metrics service.\n    enabled: true\n    external:\n      # -- Enable the external controller service or not. Useful for internal-only deployments.\n      enabled: true\n    # -- Annotations to be added to the external controller service. See `controller.service.internal.annotations` for annotations to be added to the internal controller service.\n    annotations: {}\n    # -- Labels to be added to both controller services.\n    labels: {}\n    # -- Type of the external controller service.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n    type: LoadBalancer\n    # -- Pre-defined cluster internal IP address of the external controller service. Take care of collisions with existing services.\n    # This value is immutable. Set once, it can not be changed without deleting and re-creating the service.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n    clusterIP: \"\"\n    # -- List of node IP addresses at which the external controller service is available.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\n    externalIPs: []\n    # -- Deprecated: Pre-defined IP address of the external controller service. Used by cloud providers to connect the resulting load balancer service to a pre-existing static IP.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer\n    loadBalancerIP: \"\"\n    # -- Restrict access to the external controller service. Values must be CIDRs. Allows any source address by default.\n    loadBalancerSourceRanges: []\n    # -- Load balancer class of the external controller service. Used by cloud providers to select a load balancer implementation other than the cloud provider default.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class\n    loadBalancerClass: \"\"\n    # -- Enable node port allocation for the external controller service or not. Applies to type `LoadBalancer` only.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation\n    # allocateLoadBalancerNodePorts: true\n\n    # -- External traffic policy of the external controller service. Set to \"Local\" to preserve source IP on providers supporting it.\n    # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    externalTrafficPolicy: \"\"\n    # -- Session affinity of the external controller service. Must be either \"None\" or \"ClientIP\" if set. Defaults to \"None\".\n    # Ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity\n    sessionAffinity: \"\"\n    # -- Specifies the health check node port (numeric port number) for the external controller service.\n    # If not specified, the service controller allocates a port from your cluster's node port range.\n    # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    # healthCheckNodePort: 0\n\n    # -- Represents the dual-stack capabilities of the external controller service. Possible values are SingleStack, PreferDualStack or RequireDualStack.\n    # Fields `ipFamilies` and `clusterIP` depend on the value of this field.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services\n    ipFamilyPolicy: SingleStack\n    # -- List of IP families (e.g. IPv4, IPv6) assigned to the external controller service. This field is usually assigned automatically based on cluster configuration and the `ipFamilyPolicy` field.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services\n    ipFamilies:\n      - IPv4\n    # -- Enable the HTTP listener on both controller services or not.\n    enableHttp: true\n    # -- Enable the HTTPS listener on both controller services or not.\n    enableHttps: true\n    ports:\n      # -- Port the external HTTP listener is published with.\n      http: 80\n      # -- Port the external HTTPS listener is published with.\n      https: 443\n    targetPorts:\n      # -- Port of the ingress controller the external HTTP listener is mapped to.\n      http: http\n      # -- Port of the ingress controller the external HTTPS listener is mapped to.\n      https: https\n    # -- Declare the app protocol of the external HTTP and HTTPS listeners or not. Supersedes provider-specific annotations for declaring the backend protocol.\n    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#application-protocol\n    appProtocol: true\n    nodePorts:\n      # -- Node port allocated for the external HTTP listener. If left empty, the service controller allocates one from the configured node port range.\n      http: \"\"\n      # -- Node port allocated for the external HTTPS listener. If left empty, the service controller allocates one from the configured node port range.\n      https: \"\"\n      # -- Node port mapping for external TCP listeners. If left empty, the service controller allocates them from the configured node port range.\n      # Example:\n      # tcp:\n      #   8080: 30080\n      tcp: {}\n      # -- Node port mapping for external UDP listeners. If left empty, the service controller allocates them from the configured node port range.\n      # Example:\n      # udp:\n      #   53: 30053\n      udp: {}\n    internal:\n      # -- Enable the internal controller service or not. Remember to configure `controller.service.internal.annotations` when enabling this.\n      enabled: false\n      # -- Annotations to be added to the internal controller service. Mandatory for the internal controller service to be created. Varies with the cloud service.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n      annotations: {}\n      # -- Type of the internal controller service.\n      # Defaults to the value of `controller.service.type`.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n      type: \"\"\n      # -- Pre-defined cluster internal IP address of the internal controller service. Take care of collisions with existing services.\n      # This value is immutable. Set once, it can not be changed without deleting and re-creating the service.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      clusterIP: \"\"\n      # -- List of node IP addresses at which the internal controller service is available.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\n      externalIPs: []\n      # -- Deprecated: Pre-defined IP address of the internal controller service. Used by cloud providers to connect the resulting load balancer service to a pre-existing static IP.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer\n      loadBalancerIP: \"\"\n      # -- Restrict access to the internal controller service. Values must be CIDRs. Allows any source address by default.\n      loadBalancerSourceRanges: []\n      # -- Load balancer class of the internal controller service. Used by cloud providers to select a load balancer implementation other than the cloud provider default.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class\n      loadBalancerClass: \"\"\n      # -- Enable node port allocation for the internal controller service or not. Applies to type `LoadBalancer` only.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation\n      # allocateLoadBalancerNodePorts: true\n\n      # -- External traffic policy of the internal controller service. Set to \"Local\" to preserve source IP on providers supporting it.\n      # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n      externalTrafficPolicy: \"\"\n      # -- Session affinity of the internal controller service. Must be either \"None\" or \"ClientIP\" if set. Defaults to \"None\".\n      # Ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity\n      sessionAffinity: \"\"\n      # -- Specifies the health check node port (numeric port number) for the internal controller service.\n      # If not specified, the service controller allocates a port from your cluster's node port range.\n      # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n      # healthCheckNodePort: 0\n\n      # -- Represents the dual-stack capabilities of the internal controller service. Possible values are SingleStack, PreferDualStack or RequireDualStack.\n      # Fields `ipFamilies` and `clusterIP` depend on the value of this field.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services\n      ipFamilyPolicy: SingleStack\n      # -- List of IP families (e.g. IPv4, IPv6) assigned to the internal controller service. This field is usually assigned automatically based on cluster configuration and the `ipFamilyPolicy` field.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services\n      ipFamilies:\n        - IPv4\n      ports: {}\n      # -- Port the internal HTTP listener is published with.\n      # Defaults to the value of `controller.service.ports.http`.\n      # http: 80\n      # -- Port the internal HTTPS listener is published with.\n      # Defaults to the value of `controller.service.ports.https`.\n      # https: 443\n\n      targetPorts: {}\n      # -- Port of the ingress controller the internal HTTP listener is mapped to.\n      # Defaults to the value of `controller.service.targetPorts.http`.\n      # http: http\n      # -- Port of the ingress controller the internal HTTPS listener is mapped to.\n      # Defaults to the value of `controller.service.targetPorts.https`.\n      # https: https\n\n      # -- Declare the app protocol of the internal HTTP and HTTPS listeners or not. Supersedes provider-specific annotations for declaring the backend protocol.\n      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#application-protocol\n      appProtocol: true\n      nodePorts:\n        # -- Node port allocated for the internal HTTP listener. If left empty, the service controller allocates one from the configured node port range.\n        http: \"\"\n        # -- Node port allocated for the internal HTTPS listener. If left empty, the service controller allocates one from the configured node port range.\n        https: \"\"\n        # -- Node port mapping for internal TCP listeners. If left empty, the service controller allocates them from the configured node port range.\n        # Example:\n        # tcp:\n        #   8080: 30080\n        tcp: {}\n        # -- Node port mapping for internal UDP listeners. If left empty, the service controller allocates them from the configured node port range.\n        # Example:\n        # udp:\n        #   53: 30053\n        udp: {}\n  # shareProcessNamespace enables process namespace sharing within the pod.\n  # This can be used for example to signal log rotation using `kill -USR1` from a sidecar.\n  shareProcessNamespace: false\n  # -- Additional containers to be added to the controller pod.\n  # See https://github.com/lemonldap-ng-controller/lemonldap-ng-controller as example.\n  extraContainers: []\n  #  - name: my-sidecar\n  #    image: nginx:latest\n  #  - name: lemonldap-ng-controller\n  #    image: lemonldapng/lemonldap-ng-controller:0.2.0\n  #    args:\n  #      - /lemonldap-ng-controller\n  #      - --alsologtostderr\n  #      - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration\n  #    env:\n  #      - name: POD_NAME\n  #        valueFrom:\n  #          fieldRef:\n  #            fieldPath: metadata.name\n  #      - name: POD_NAMESPACE\n  #        valueFrom:\n  #          fieldRef:\n  #            fieldPath: metadata.namespace\n  #    volumeMounts:\n  #    - name: copy-portal-skins\n  #      mountPath: /srv/var/lib/lemonldap-ng/portal/skins\n\n  # -- Additional volumeMounts to the controller main container.\n  extraVolumeMounts: []\n  #  - name: copy-portal-skins\n  #   mountPath: /var/lib/lemonldap-ng/portal/skins\n\n  # -- Additional volumes to the controller pod.\n  extraVolumes: []\n  #  - name: copy-portal-skins\n  #    emptyDir: {}\n\n  # -- Containers, which are run before the app containers are started.\n  extraInitContainers: []\n  # - name: init-myservice\n  #   image: busybox\n  #   command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n\n  # -- Modules, which are mounted into the core nginx image. See values.yaml for a sample to add opentelemetry module\n  extraModules: []\n  # - name: mytestmodule\n  #   image:\n  #     registry: registry.k8s.io\n  #     image: ingress-nginx/mytestmodule\n  #     ## for backwards compatibility consider setting the full image url via the repository value below\n  #     ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail\n  #     ## repository:\n  #     tag: \"v1.0.0\"\n  #     digest: \"\"\n  #     distroless: false\n  #   containerSecurityContext:\n  #     runAsNonRoot: true\n  #     runAsUser: \u003cuser-id\u003e\n  #     allowPrivilegeEscalation: false\n  #     seccompProfile:\n  #       type: RuntimeDefault\n  #     capabilities:\n  #       drop:\n  #       - ALL\n  #     readOnlyRootFilesystem: true\n  #   resources: {}\n  #\n  # The image must contain a `/usr/local/bin/init_module.sh` executable, which\n  # will be executed as initContainers, to move its config files within the\n  # mounted volume.\n\n  opentelemetry:\n    enabled: false\n    name: opentelemetry\n    image:\n      registry: registry.k8s.io\n      image: ingress-nginx/opentelemetry\n      ## for backwards compatibility consider setting the full image url via the repository value below\n      ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail\n      ## repository:\n      tag: \"v20230721-3e2062ee5\"\n      digest: sha256:13bee3f5223883d3ca62fee7309ad02d22ec00ff0d7033e3e9aca7a9f60fd472\n      distroless: true\n    containerSecurityContext:\n      runAsNonRoot: true\n      # -- The image's default user, inherited from its base image `cgr.dev/chainguard/static`.\n      runAsUser: 65532\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: RuntimeDefault\n      capabilities:\n        drop:\n          - ALL\n      readOnlyRootFilesystem: true\n    resources: {}\n  admissionWebhooks:\n    name: admission\n    annotations: {}\n    # ignore-check.kube-linter.io/no-read-only-rootfs: \"This deployment needs write access to root filesystem\".\n\n    ## Additional annotations to the admission webhooks.\n    ## These annotations will be added to the ValidatingWebhookConfiguration and\n    ## the Jobs Spec of the admission webhooks.\n    enabled: true\n    # -- Additional environment variables to set\n    extraEnvs: []\n    # extraEnvs:\n    #   - name: FOO\n    #     valueFrom:\n    #       secretKeyRef:\n    #         key: FOO\n    #         name: secret-resource\n    # -- Admission Webhook failure policy to use\n    failurePolicy: Fail\n    # timeoutSeconds: 10\n    port: 8443\n    certificate: \"/usr/local/certificates/cert\"\n    key: \"/usr/local/certificates/key\"\n    namespaceSelector: {}\n    objectSelector: {}\n    # -- Labels to be added to admission webhooks\n    labels: {}\n    # -- Use an existing PSP instead of creating one\n    existingPsp: \"\"\n    service:\n      annotations: {}\n      # clusterIP: \"\"\n      externalIPs: []\n      # loadBalancerIP: \"\"\n      loadBalancerSourceRanges: []\n      servicePort: 443\n      type: ClusterIP\n    createSecretJob:\n      name: create\n      # -- Security context for secret creation containers\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65532\n        allowPrivilegeEscalation: false\n        seccompProfile:\n          type: RuntimeDefault\n        capabilities:\n          drop:\n            - ALL\n        readOnlyRootFilesystem: true\n      resources: {}\n      # limits:\n      #   cpu: 10m\n      #   memory: 20Mi\n      # requests:\n      #   cpu: 10m\n      #   memory: 20Mi\n    patchWebhookJob:\n      name: patch\n      # -- Security context for webhook patch containers\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65532\n        allowPrivilegeEscalation: false\n        seccompProfile:\n          type: RuntimeDefault\n        capabilities:\n          drop:\n            - ALL\n        readOnlyRootFilesystem: true\n      resources: {}\n    patch:\n      enabled: true\n      image:\n        registry: registry.k8s.io\n        image: ingress-nginx/kube-webhook-certgen\n        ## for backwards compatibility consider setting the full image url via the repository value below\n        ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail\n        ## repository:\n        tag: v1.4.1\n        digest: sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\n        pullPolicy: IfNotPresent\n      # -- Provide a priority class name to the webhook patching job\n      ##\n      priorityClassName: \"\"\n      podAnnotations: {}\n      # NetworkPolicy for webhook patch\n      networkPolicy:\n        # -- Enable 'networkPolicy' or not\n        enabled: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations: []\n      # -- Labels to be added to patch job resources\n      labels: {}\n      # -- Security context for secret creation \u0026 webhook patch pods\n      securityContext: {}\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false\n      # self-signed root certificate\n      rootCert:\n        # default to be 5y\n        duration: \"\"\n      admissionCert:\n        # default to be 1y\n        duration: \"\"\n        # issuerRef:\n        #   name: \"issuer\"\n        #   kind: \"ClusterIssuer\"\n  metrics:\n    port: 10254\n    portName: metrics\n    # if this port is changed, change healthz-port: in extraArgs: accordingly\n    enabled: false\n    service:\n      annotations: {}\n      # prometheus.io/scrape: \"true\"\n      # prometheus.io/port: \"10254\"\n      # -- Labels to be added to the metrics service resource\n      labels: {}\n      # clusterIP: \"\"\n\n      # -- List of IP addresses at which the stats-exporter service is available\n      ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\n      ##\n      externalIPs: []\n      # loadBalancerIP: \"\"\n      loadBalancerSourceRanges: []\n      servicePort: 10254\n      type: ClusterIP\n      # externalTrafficPolicy: \"\"\n      # nodePort: \"\"\n    serviceMonitor:\n      enabled: false\n      additionalLabels: {}\n      annotations: {}\n      ## The label to use to retrieve the job name from.\n      ## jobLabel: \"app.kubernetes.io/name\"\n      namespace: \"\"\n      namespaceSelector: {}\n      ## Default: scrape .Release.Namespace or namespaceOverride only\n      ## To scrape all, use the following:\n      ## namespaceSelector:\n      ##   any: true\n      scrapeInterval: 30s\n      # honorLabels: true\n      targetLabels: []\n      relabelings: []\n      metricRelabelings: []\n    prometheusRule:\n      enabled: false\n      additionalLabels: {}\n      # namespace: \"\"\n      rules: []\n      # # These are just examples rules, please adapt them to your needs\n      # - alert: NGINXConfigFailed\n      #   expr: count(nginx_ingress_controller_config_last_reload_successful == 0) \u003e 0\n      #   for: 1s\n      #   labels:\n      #     severity: critical\n      #   annotations:\n      #     description: bad ingress config - nginx config test failed\n      #     summary: uninstall the latest ingress changes to allow config reloads to resume\n      # # By default a fake self-signed certificate is generated as default and\n      # # it is fine if it expires. If `--default-ssl-certificate` flag is used\n      # # and a valid certificate passed please do not filter for `host` label!\n      # # (i.e. delete `{host!=\"_\"}` so also the default SSL certificate is\n      # # checked for expiration)\n      # - alert: NGINXCertificateExpiry\n      #   expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds{host!=\"_\"}) by (host) - time()) \u003c 604800\n      #   for: 1s\n      #   labels:\n      #     severity: critical\n      #   annotations:\n      #     description: ssl certificate(s) will expire in less then a week\n      #     summary: renew expiring certificates to avoid downtime\n      # - alert: NGINXTooMany500s\n      #   expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) \u003e 5\n      #   for: 1m\n      #   labels:\n      #     severity: warning\n      #   annotations:\n      #     description: Too many 5XXs\n      #     summary: More than 5% of all requests returned 5XX, this requires your attention\n      # - alert: NGINXTooMany400s\n      #   expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) \u003e 5\n      #   for: 1m\n      #   labels:\n      #     severity: warning\n      #   annotations:\n      #     description: Too many 4XXs\n      #     summary: More than 5% of all requests returned 4XX, this requires your attention\n  # -- Improve connection draining when ingress controller pod is deleted using a lifecycle hook:\n  # With this new hook, we increased the default terminationGracePeriodSeconds from 30 seconds\n  # to 300, allowing the draining of connections up to five minutes.\n  # If the active connections end before that, the pod will terminate gracefully at that time.\n  # To effectively take advantage of this feature, the Configmap feature\n  # worker-shutdown-timeout new value is 240s instead of 10s.\n  ##\n  lifecycle:\n    preStop:\n      exec:\n        command:\n          - /wait-shutdown\n  priorityClassName: \"\"\n# -- Rollback limit\n##\nrevisionHistoryLimit: 10\n## Default 404 backend\n##\ndefaultBackend:\n  ##\n  enabled: false\n  name: defaultbackend\n  image:\n    registry: registry.k8s.io\n    image: defaultbackend-amd64\n    ## for backwards compatibility consider setting the full image url via the repository value below\n    ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail\n    ## repository:\n    tag: \"1.5\"\n    pullPolicy: IfNotPresent\n    runAsNonRoot: true\n    # nobody user -\u003e uid 65534\n    runAsUser: 65534\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    readOnlyRootFilesystem: true\n  # -- Use an existing PSP instead of creating one\n  existingPsp: \"\"\n  extraArgs: {}\n  serviceAccount:\n    create: true\n    name: \"\"\n    automountServiceAccountToken: true\n  # -- Additional environment variables to set for defaultBackend pods\n  extraEnvs: []\n  port: 8080\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  ##\n  livenessProbe:\n    failureThreshold: 3\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 5\n  readinessProbe:\n    failureThreshold: 6\n    initialDelaySeconds: 0\n    periodSeconds: 5\n    successThreshold: 1\n    timeoutSeconds: 5\n  # -- The update strategy to apply to the Deployment or DaemonSet\n  ##\n  updateStrategy: {}\n  #  rollingUpdate:\n  #    maxUnavailable: 1\n  #  type: RollingUpdate\n\n  # -- `minReadySeconds` to avoid killing pods before we are ready\n  ##\n  minReadySeconds: 0\n  # -- Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n  #  - key: \"key\"\n  #    operator: \"Equal|Exists\"\n  #    value: \"value\"\n  #    effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  affinity: {}\n  # -- Security context for default backend pods\n  podSecurityContext: {}\n  # -- Security context for default backend containers\n  containerSecurityContext: {}\n  # -- Labels to add to the pod container metadata\n  podLabels: {}\n  #  key: value\n\n  # -- Node labels for default backend pod assignment\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\n  ##\n  nodeSelector:\n    kubernetes.io/os: linux\n  # -- Annotations to be added to default backend pods\n  ##\n  podAnnotations: {}\n  replicaCount: 1\n  minAvailable: 1\n  resources: {}\n  # limits:\n  #   cpu: 10m\n  #   memory: 20Mi\n  # requests:\n  #   cpu: 10m\n  #   memory: 20Mi\n\n  extraVolumeMounts: []\n  ## Additional volumeMounts to the default backend container.\n  #  - name: copy-portal-skins\n  #   mountPath: /var/lib/lemonldap-ng/portal/skins\n\n  extraVolumes: []\n  ## Additional volumes to the default backend pod.\n  #  - name: copy-portal-skins\n  #    emptyDir: {}\n\n  extraConfigMaps: []\n  ## Additional configmaps to the default backend pod.\n  #  - name: my-extra-configmap-1\n  #    labels:\n  #      type: config-1\n  #    data:\n  #      extra_file_1.html: |\n  #        \u003c!-- Extra HTML content for ConfigMap 1 --\u003e\n  #  - name: my-extra-configmap-2\n  #    labels:\n  #      type: config-2\n  #    data:\n  #      extra_file_2.html: |\n  #        \u003c!-- Extra HTML content for ConfigMap 2 --\u003e\n\n  autoscaling:\n    annotations: {}\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 2\n    targetCPUUtilizationPercentage: 50\n    targetMemoryUtilizationPercentage: 50\n  # NetworkPolicy for default backend component.\n  networkPolicy:\n    # -- Enable 'networkPolicy' or not\n    enabled: false\n  service:\n    annotations: {}\n    # clusterIP: \"\"\n\n    # -- List of IP addresses at which the default backend service is available\n    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\n    ##\n    externalIPs: []\n    # loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    type: ClusterIP\n  priorityClassName: \"\"\n  # -- Labels to be added to the default backend resources\n  labels: {}\n## Enable RBAC as per https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/rbac.md and https://github.com/kubernetes/ingress-nginx/issues/266\nrbac:\n  create: true\n  scope: false\n## If true, create \u0026 use Pod Security Policy resources\n## https://kubernetes.io/docs/concepts/policy/pod-security-policy/\npodSecurityPolicy:\n  enabled: false\nserviceAccount:\n  create: true\n  name: \"\"\n  automountServiceAccountToken: true\n  # -- Annotations for the controller service account\n  annotations: {}\n# -- Optional array of imagePullSecrets containing private registry credentials\n## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# - name: secretName\n\n# -- TCP service key-value pairs\n## Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/exposing-tcp-udp-services.md\n##\ntcp: {}\n#  \"8080\": \"default/example-tcp-svc:9000\"\n\n# -- UDP service key-value pairs\n## Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/exposing-tcp-udp-services.md\n##\nudp: {}\n#  \"53\": \"kube-system/kube-dns:53\"\n\n# -- Prefix for TCP and UDP ports names in ingress controller service\n## Some cloud providers, like Yandex Cloud may have a requirements for a port name regex to support cloud load balancer integration\nportNamePrefix: \"\"\n# -- (string) A base64-encoded Diffie-Hellman parameter.\n# This can be generated with: `openssl dhparam 4096 2\u003e /dev/null | base64`\n## Ref: https://github.com/kubernetes/ingress-nginx/tree/main/docs/examples/customization/ssl-dh-param\ndhParam: \"\"\n"],"verify":false,"version":"4.10.3","wait":true,"wait_for_jobs":false},"sensitive_attributes":[[{"type":"get_attr","value":"repository_password"}]],"private":"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==","dependencies":["kubernetes_namespace.ingress-nginx_ns"]}]},{"mode":"managed","type":"helm_release","name":"loki-stack","provider":"provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"schema_version":1,"attributes":{"atomic":false,"chart":"loki-stack","cleanup_on_fail":false,"create_namespace":false,"dependency_update":false,"description":null,"devel":null,"disable_crd_hooks":false,"disable_openapi_validation":false,"disable_webhooks":false,"force_update":false,"id":"loki-name","keyring":null,"lint":false,"manifest":null,"max_history":0,"metadata":[{"app_version":"v2.9.3","chart":"loki-stack","first_deployed":1722489218,"last_deployed":1722489218,"name":"loki-name","namespace":"loki-stack","notes":"***********************************************************************\n Welcome to Grafana Promtail\n Chart version: 6.15.5\n Promtail version: 2.9.3\n***********************************************************************\n\nVerify the application is working by running these commands:\n* kubectl --namespace loki-stack port-forward daemonset/loki-name-promtail 3101\n* curl http://127.0.0.1:3101/metrics\n\n1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace loki-stack loki-name-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   loki-name-grafana.loki-stack.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\n\n     export POD_NAME=$(kubectl get pods --namespace loki-stack -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=loki-name\" -o jsonpath=\"{.items[0].metadata.name}\")\n     kubectl --namespace loki-stack port-forward $POD_NAME 3000\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n\nThe Loki stack has been deployed to your cluster. Loki can now be added as a datasource in Grafana.\n\nSee http://docs.grafana.org/features/datasources/loki/ for more detail.\n\nVerify the application is working by running these commands:\n  kubectl --namespace loki-stack port-forward service/loki-name 3100\n  curl http://127.0.0.1:3100/api/prom/label\n","revision":1,"values":"{\"filebeat\":{\"enabled\":false,\"filebeatConfig\":{\"filebeat.yml\":\"# logging.level: debug\\nfilebeat.inputs:\\n- type: container\\n  paths:\\n    - /var/log/containers/*.log\\n  processors:\\n  - add_kubernetes_metadata:\\n      host: ${NODE_NAME}\\n      matchers:\\n      - logs_path:\\n          logs_path: \\\"/var/log/containers/\\\"\\noutput.logstash:\\n  hosts: [\\\"logstash-loki:5044\\\"]\\n\"}},\"fluent-bit\":{\"enabled\":false},\"grafana\":{\"enabled\":true,\"grafana.ini\":{\"server\":{\"domain\":\"192.168.56.101:30232\",\"root_url\":\"http://192.168.58.101:31545/loki\",\"serve_from_sub_path\":true}},\"image\":{\"tag\":\"10.3.3\"},\"sidecar\":{\"datasources\":{\"enabled\":true,\"label\":\"\",\"labelValue\":\"\",\"maxLines\":2000}}},\"logstash\":{\"enabled\":false,\"filters\":{\"main\":\"filter {\\n  if [kubernetes] {\\n    mutate {\\n      add_field =\\u003e {\\n        \\\"container_name\\\" =\\u003e \\\"%{[kubernetes][container][name]}\\\"\\n        \\\"namespace\\\" =\\u003e \\\"%{[kubernetes][namespace]}\\\"\\n        \\\"pod\\\" =\\u003e \\\"%{[kubernetes][pod][name]}\\\"\\n      }\\n      replace =\\u003e { \\\"host\\\" =\\u003e \\\"%{[kubernetes][node][name]}\\\"}\\n    }\\n  }\\n  mutate {\\n    remove_field =\\u003e [\\\"tags\\\"]\\n  }\\n}\"},\"image\":\"grafana/logstash-output-loki\",\"imageTag\":\"1.0.1\",\"outputs\":{\"main\":\"output {\\n  loki {\\n    url =\\u003e \\\"http://loki:3100/loki/api/v1/push\\\"\\n    #username =\\u003e \\\"test\\\"\\n    #password =\\u003e \\\"test\\\"\\n  }\\n  # stdout { codec =\\u003e rubydebug }\\n}\"}},\"loki\":{\"datasource\":{\"jsonData\":\"{}\",\"uid\":\"\"},\"enabled\":true,\"isDefault\":true,\"livenessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":45},\"url\":\"http://{{(include \\\"loki.serviceName\\\" .)}}:{{ .Values.loki.service.port }}\"},\"prometheus\":{\"datasource\":{\"jsonData\":\"{}\"},\"enabled\":false,\"isDefault\":false,\"url\":\"http://{{ include \\\"prometheus.fullname\\\" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}\"},\"promtail\":{\"config\":{\"clients\":[{\"url\":\"http://{{ .Release.Name }}:3100/loki/api/v1/push\"}],\"logLevel\":\"info\",\"serverPort\":3101},\"enabled\":true},\"proxy\":{\"http_proxy\":\"\",\"https_proxy\":\"\",\"no_proxy\":\"\"},\"test_pod\":{\"enabled\":true,\"image\":\"bats/bats:1.8.2\",\"pullPolicy\":\"IfNotPresent\"}}","version":"2.10.2"}],"name":"loki-name","namespace":"loki-stack","pass_credentials":false,"postrender":[],"recreate_pods":false,"render_subchart_notes":true,"replace":false,"repository":"./helm/","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":[{"name":"grafana.enabled","type":"","value":"true"},{"name":"grafana.grafana\\.ini.server.domain","type":"","value":"192.168.56.101:30232"},{"name":"grafana.grafana\\.ini.server.root_url","type":"","value":"http://192.168.58.101:31545/loki"},{"name":"grafana.grafana\\.ini.server.serve_from_sub_path","type":"","value":"true"},{"name":"grafana.sidecar.datasources.maxLines","type":"","value":"2000"}],"set_list":[],"set_sensitive":[],"skip_crds":false,"status":"deployed","timeout":300,"values":["test_pod:\n  enabled: true\n  image: bats/bats:1.8.2\n  pullPolicy: IfNotPresent\n\nloki:\n  enabled: true\n  isDefault: true\n  url: http://{{(include \"loki.serviceName\" .)}}:{{ .Values.loki.service.port }}\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  livenessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 45\n  datasource:\n    jsonData: \"{}\"\n    uid: \"\"\n\n\npromtail:\n  enabled: true\n  config:\n    logLevel: info\n    serverPort: 3101\n    clients:\n      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push\n\nfluent-bit:\n  enabled: false\n\ngrafana:\n  enabled: false\n  sidecar:\n    datasources:\n      label: \"\"\n      labelValue: \"\"\n      enabled: true\n      maxLines: 1000\n  image:\n    tag: 10.3.3\n\n#  grafana.ini:\n#    server:\n#      domain: \"192.168.56.101:30232\"\n#      root_url: \"http://192.168.56.101:30232/loki\"\n#      serve_form_sub_path: true\n\nprometheus:\n  enabled: false\n  isDefault: false\n  url: http://{{ include \"prometheus.fullname\" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}\n  datasource:\n    jsonData: \"{}\"\n\nfilebeat:\n  enabled: false\n  filebeatConfig:\n    filebeat.yml: |\n      # logging.level: debug\n      filebeat.inputs:\n      - type: container\n        paths:\n          - /var/log/containers/*.log\n        processors:\n        - add_kubernetes_metadata:\n            host: ${NODE_NAME}\n            matchers:\n            - logs_path:\n                logs_path: \"/var/log/containers/\"\n      output.logstash:\n        hosts: [\"logstash-loki:5044\"]\n\nlogstash:\n  enabled: false\n  image: grafana/logstash-output-loki\n  imageTag: 1.0.1\n  filters:\n    main: |-\n      filter {\n        if [kubernetes] {\n          mutate {\n            add_field =\u003e {\n              \"container_name\" =\u003e \"%{[kubernetes][container][name]}\"\n              \"namespace\" =\u003e \"%{[kubernetes][namespace]}\"\n              \"pod\" =\u003e \"%{[kubernetes][pod][name]}\"\n            }\n            replace =\u003e { \"host\" =\u003e \"%{[kubernetes][node][name]}\"}\n          }\n        }\n        mutate {\n          remove_field =\u003e [\"tags\"]\n        }\n      }\n  outputs:\n    main: |-\n      output {\n        loki {\n          url =\u003e \"http://loki:3100/loki/api/v1/push\"\n          #username =\u003e \"test\"\n          #password =\u003e \"test\"\n        }\n        # stdout { codec =\u003e rubydebug }\n      }\n\n# proxy is currently only used by loki test pod\n# Note: If http_proxy/https_proxy are set, then no_proxy should include the\n# loki service name, so that tests are able to communicate with the loki\n# service.\nproxy:\n  http_proxy: \"\"\n  https_proxy: \"\"\n  no_proxy: \"\"\n"],"verify":false,"version":"2.10.2","wait":true,"wait_for_jobs":false},"sensitive_attributes":[[{"type":"get_attr","value":"repository_password"}]],"private":"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==","dependencies":["kubernetes_namespace.loki_ns"]}]},{"mode":"managed","type":"helm_release","name":"metrics-server","provider":"provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"schema_version":1,"attributes":{"atomic":false,"chart":"metrics-server","cleanup_on_fail":false,"create_namespace":false,"dependency_update":false,"description":null,"devel":null,"disable_crd_hooks":false,"disable_openapi_validation":false,"disable_webhooks":false,"force_update":false,"id":"metrics-server","keyring":null,"lint":false,"manifest":null,"max_history":0,"metadata":[{"app_version":"0.7.1","chart":"metrics-server","first_deployed":1722237594,"last_deployed":1722237594,"name":"metrics-server","namespace":"kube-system","notes":"***********************************************************************\n* Metrics Server                                                      *\n***********************************************************************\n  Chart version: 3.12.1\n  App version:   0.7.1\n  Image tag:     registry.k8s.io/metrics-server/metrics-server:v0.7.1\n***********************************************************************\n","revision":1,"values":"{\"addonResizer\":{\"enabled\":false,\"image\":{\"repository\":\"registry.k8s.io/autoscaling/addon-resizer\",\"tag\":\"1.8.20\"},\"nanny\":{\"cpu\":\"0m\",\"extraCpu\":\"1m\",\"extraMemory\":\"2Mi\",\"memory\":\"0Mi\",\"minClusterSize\":100,\"pollPeriod\":300000,\"threshold\":5},\"resources\":{\"limits\":{\"cpu\":\"40m\",\"memory\":\"25Mi\"},\"requests\":{\"cpu\":\"40m\",\"memory\":\"25Mi\"}},\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}}},\"affinity\":{},\"apiService\":{\"annotations\":{},\"caBundle\":\"\",\"create\":true,\"insecureSkipTLSVerify\":true},\"args\":[\"--kubelet-insecure-tls\"],\"commonLabels\":{},\"containerPort\":10250,\"defaultArgs\":[\"--cert-dir=/tmp\",\"--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\",\"--kubelet-use-node-status-port\",\"--metric-resolution=15s\"],\"deploymentAnnotations\":{},\"dnsConfig\":{},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"hostNetwork\":{\"enabled\":false},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"registry.k8s.io/metrics-server/metrics-server\",\"tag\":\"\"},\"imagePullSecrets\":[],\"livenessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/livez\",\"port\":\"https\",\"scheme\":\"HTTPS\"},\"initialDelaySeconds\":0,\"periodSeconds\":10},\"metrics\":{\"enabled\":false},\"nameOverride\":\"\",\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":null,\"minAvailable\":null},\"podLabels\":{},\"podSecurityContext\":{},\"priorityClassName\":\"system-cluster-critical\",\"rbac\":{\"create\":true,\"pspEnabled\":false},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"/readyz\",\"port\":\"https\",\"scheme\":\"HTTPS\"},\"initialDelaySeconds\":20,\"periodSeconds\":10},\"replicas\":1,\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"200Mi\"}},\"revisionHistoryLimit\":null,\"schedulerName\":\"\",\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"annotations\":{},\"labels\":{},\"port\":443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\",\"secrets\":[]},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"1m\",\"metricRelabelings\":[],\"relabelings\":[],\"scrapeTimeout\":\"10s\"},\"tmpVolume\":{\"emptyDir\":{}},\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":{}}","version":"3.12.1"}],"name":"metrics-server","namespace":"kube-system","pass_credentials":false,"postrender":[],"recreate_pods":true,"render_subchart_notes":true,"replace":false,"repository":"./helm","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":[],"set_list":[{"name":"args","value":["--kubelet-insecure-tls"]}],"set_sensitive":[],"skip_crds":false,"status":"deployed","timeout":300,"values":["# Default values for metrics-server.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nimage:\n  repository: registry.k8s.io/metrics-server/metrics-server\n  # Overrides the image tag whose default is v{{ .Chart.AppVersion }}\n  tag: \"\"\n  pullPolicy: IfNotPresent\n\nimagePullSecrets: []\n# - name: registrySecretName\n\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n  # The list of secrets mountable by this service account.\n  # See https://kubernetes.io/docs/reference/labels-annotations-taints/#enforce-mountable-secrets\n  secrets: []\n\nrbac:\n  # Specifies whether RBAC resources should be created\n  create: true\n  pspEnabled: false\n\napiService:\n  # Specifies if the v1beta1.metrics.k8s.io API service should be created.\n  #\n  # You typically want this enabled! If you disable API service creation you have to\n  # manage it outside of this chart for e.g horizontal pod autoscaling to\n  # work with this release.\n  create: true\n  # Annotations to add to the API service\n  annotations: {}\n  # Specifies whether to skip TLS verification\n  insecureSkipTLSVerify: true\n  # The PEM encoded CA bundle for TLS verification\n  caBundle: \"\"\n\ncommonLabels: {}\npodLabels: {}\npodAnnotations: {}\n\npodSecurityContext: {}\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n  seccompProfile:\n    type: RuntimeDefault\n  capabilities:\n    drop:\n      - ALL\n\npriorityClassName: system-cluster-critical\n\ncontainerPort: 10250\n\nhostNetwork:\n  # Specifies if metrics-server should be started in hostNetwork mode.\n  #\n  # You would require this enabled if you use alternate overlay networking for pods and\n  # API server unable to communicate with metrics-server. As an example, this is required\n  # if you use Weave network on EKS\n  enabled: false\n\nreplicas: 1\n\nrevisionHistoryLimit:\n\nupdateStrategy: {}\n#   type: RollingUpdate\n#   rollingUpdate:\n#     maxSurge: 0\n#     maxUnavailable: 1\n\npodDisruptionBudget:\n  # https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  enabled: false\n  minAvailable:\n  maxUnavailable:\n\ndefaultArgs:\n  - --cert-dir=/tmp\n  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n  - --kubelet-use-node-status-port\n  - --metric-resolution=15s\n\nargs: []\n\nlivenessProbe:\n  httpGet:\n    path: /livez\n    port: https\n    scheme: HTTPS\n  initialDelaySeconds: 0\n  periodSeconds: 10\n  failureThreshold: 3\n\nreadinessProbe:\n  httpGet:\n    path: /readyz\n    port: https\n    scheme: HTTPS\n  initialDelaySeconds: 20\n  periodSeconds: 10\n  failureThreshold: 3\n\nservice:\n  type: ClusterIP\n  port: 443\n  annotations: {}\n  labels: {}\n  #  Add these labels to have metrics-server show up in `kubectl cluster-info`\n  #  kubernetes.io/cluster-service: \"true\"\n  #  kubernetes.io/name: \"Metrics-server\"\n\naddonResizer:\n  enabled: false\n  image:\n    repository: registry.k8s.io/autoscaling/addon-resizer\n    tag: 1.8.20\n  securityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    runAsNonRoot: true\n    runAsUser: 1000\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n        - ALL\n  resources:\n    requests:\n      cpu: 40m\n      memory: 25Mi\n    limits:\n      cpu: 40m\n      memory: 25Mi\n  nanny:\n    cpu: 0m\n    extraCpu: 1m\n    memory: 0Mi\n    extraMemory: 2Mi\n    minClusterSize: 100\n    pollPeriod: 300000\n    threshold: 5\n\nmetrics:\n  enabled: false\n\nserviceMonitor:\n  enabled: false\n  additionalLabels: {}\n  interval: 1m\n  scrapeTimeout: 10s\n  metricRelabelings: []\n  relabelings: []\n\n# See https://github.com/kubernetes-sigs/metrics-server#scaling\nresources:\n  requests:\n    cpu: 100m\n    memory: 200Mi\n  # limits:\n  #   cpu:\n  #   memory:\n\nextraVolumeMounts: []\n\nextraVolumes: []\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\ntopologySpreadConstraints: []\n\ndnsConfig: {}\n\n# Annotations to add to the deployment\ndeploymentAnnotations: {}\n\nschedulerName: \"\"\n\ntmpVolume:\n  emptyDir: {}\n"],"verify":false,"version":"3.12.1","wait":true,"wait_for_jobs":false},"sensitive_attributes":[[{"type":"get_attr","value":"repository_password"}]],"private":"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="}]},{"mode":"managed","type":"helm_release","name":"prometheus","provider":"provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"schema_version":1,"attributes":{"atomic":false,"chart":"kube-prometheus-stack","cleanup_on_fail":false,"create_namespace":false,"dependency_update":false,"description":null,"devel":null,"disable_crd_hooks":false,"disable_openapi_validation":false,"disable_webhooks":false,"force_update":false,"id":"prometheus","keyring":null,"lint":false,"manifest":null,"max_history":0,"metadata":[{"app_version":"v0.75.1","chart":"kube-prometheus-stack","first_deployed":1722240797,"last_deployed":1722411982,"name":"prometheus","namespace":"prometheus","notes":"kube-state-metrics is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.\nThe exposed metrics can be found here:\nhttps://github.com/kubernetes/kube-state-metrics/blob/master/docs/README.md#exposed-metrics\n\nThe metrics are exported on the HTTP endpoint /metrics on the listening port.\nIn your case, prometheus-kube-state-metrics.prometheus.svc.cluster.local:8080/metrics\n\nThey are served either as plaintext or protobuf depending on the Accept header.\nThey are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint.\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace prometheus -l \"app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9100 to use your application\"\n  kubectl port-forward --namespace prometheus $POD_NAME 9100\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace prometheus get pods -l \"release=prometheus\"\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026 configure Alertmanager and Prometheus instances using the Operator.\n","revision":3,"values":"{\"additionalPrometheusRulesMap\":{},\"alertmanager\":{\"alertmanagerSpec\":{\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPeers\":[],\"affinity\":{},\"alertmanagerConfigMatcherStrategy\":{},\"alertmanagerConfigNamespaceSelector\":{},\"alertmanagerConfigSelector\":{},\"alertmanagerConfiguration\":{},\"automountServiceAccountToken\":true,\"clusterAdvertiseAddress\":false,\"clusterGossipInterval\":\"\",\"clusterPeerTimeout\":\"\",\"clusterPushpullInterval\":\"\",\"configMaps\":[],\"containers\":[],\"externalUrl\":null,\"forceEnableClusterMode\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/alertmanager\",\"sha\":\"\",\"tag\":\"v0.27.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"minReadySeconds\":0,\"nodeSelector\":{},\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"replicas\":1,\"resources\":{},\"retention\":\"120h\",\"routePrefix\":\"/\",\"scheme\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tlsConfig\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useExistingSecret\":false,\"volumeMounts\":[],\"volumes\":[],\"web\":{}},\"annotations\":{},\"apiVersion\":\"v2\",\"config\":{\"global\":{\"resolve_timeout\":\"5m\"},\"inhibit_rules\":[{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = critical\"],\"target_matchers\":[\"severity =~ warning|info\"]},{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = warning\"],\"target_matchers\":[\"severity = info\"]},{\"equal\":[\"namespace\"],\"source_matchers\":[\"alertname = InfoInhibitor\"],\"target_matchers\":[\"severity = info\"]},{\"target_matchers\":[\"alertname = InfoInhibitor\"]}],\"receivers\":[{\"name\":\"null\"}],\"route\":{\"group_by\":[\"namespace\"],\"group_interval\":\"5m\",\"group_wait\":\"30s\",\"receiver\":\"null\",\"repeat_interval\":\"12h\",\"routes\":[{\"matchers\":[\"alertname = \\\"Watchdog\\\"\"],\"receiver\":\"null\"}]},\"templates\":[\"/etc/alertmanager/config/*.tmpl\"]},\"enableFeatures\":[],\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"alertmanager\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"secret\":{\"annotations\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30903,\"port\":9093,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9093,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"enableHttp2\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerSourceRanges\":[],\"nodePort\":30904,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"stringConfig\":\"\",\"templateFiles\":{},\"tplConfig\":false},\"cleanPrometheusOperatorObjectNames\":false,\"commonLabels\":{},\"coreDns\":{\"enabled\":true,\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLimit\":0}},\"crds\":{\"enabled\":true},\"customRules\":{},\"defaultRules\":{\"additionalAggregationLabels\":[],\"additionalRuleAnnotations\":{},\"additionalRuleGroupAnnotations\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleGroupLabels\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleLabels\":{},\"annotations\":{},\"appNamespacesTarget\":\".*\",\"create\":true,\"disabled\":{},\"keepFiringFor\":\"\",\"labels\":{},\"rules\":{\"alertmanager\":true,\"configReloaders\":true,\"etcd\":true,\"general\":true,\"k8sContainerCpuUsageSecondsTotal\":true,\"k8sContainerMemoryCache\":true,\"k8sContainerMemoryRss\":true,\"k8sContainerMemorySwap\":true,\"k8sContainerMemoryWorkingSetBytes\":true,\"k8sContainerResource\":true,\"k8sPodOwner\":true,\"kubeApiserverAvailability\":true,\"kubeApiserverBurnrate\":true,\"kubeApiserverHistogram\":true,\"kubeApiserverSlos\":true,\"kubeControllerManager\":true,\"kubePrometheusGeneral\":true,\"kubePrometheusNodeRecording\":true,\"kubeProxy\":true,\"kubeSchedulerAlerting\":true,\"kubeSchedulerRecording\":true,\"kubeStateMetrics\":true,\"kubelet\":true,\"kubernetesApps\":true,\"kubernetesResources\":true,\"kubernetesStorage\":true,\"kubernetesSystem\":true,\"network\":true,\"node\":true,\"nodeExporterAlerting\":true,\"nodeExporterRecording\":true,\"prometheus\":true,\"prometheusOperator\":true,\"windows\":true},\"runbookUrl\":\"https://runbooks.prometheus-operator.dev/runbooks\"},\"extraManifests\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"rbac\":{\"create\":true,\"createAggregateClusterRoles\":false,\"pspAnnotations\":{},\"pspEnabled\":false}},\"grafana\":{\"additionalDataSources\":[],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEditable\":true,\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":false,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"grafana.ini\":{\"server\":{\"domain\":\"192.168.56.101:30232\",\"root_url\":\"http://192.168.58.101:31545/grafana\",\"serve_from_sub_path\":true}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"ingressClassName\":\"nginx\",\"labels\":{},\"path\":\"/grafana\",\"tls\":[]},\"namespaceOverride\":\"\",\"rbac\":{\"pspEnabled\":false},\"service\":{\"ipFamilies\":[],\"ipFamilyPolicy\":\"\",\"portName\":\"http-web\"},\"serviceAccount\":{\"autoMount\":true,\"create\":true},\"serviceMonitor\":{\"enabled\":true,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enableNewTablePanelSyntax\":false,\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"1\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false},\"searchNamespace\":\"ALL\"},\"datasources\":{\"alertmanager\":{\"enabled\":true,\"handleGrafanaManagedAlerts\":false,\"implementation\":\"prometheus\",\"name\":\"Alertmanager\",\"uid\":\"alertmanager\"},\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"exemplarTraceIdDestinations\":{},\"httpMethod\":\"POST\",\"isDefaultDatasource\":true,\"label\":\"grafana_datasource\",\"labelValue\":\"1\",\"name\":\"Prometheus\",\"uid\":\"prometheus\"}}},\"kube-state-metrics\":{\"namespaceOverride\":\"\",\"prometheus\":{\"monitor\":{\"enabled\":true,\"honorLabels\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"create\":true},\"releaseLabel\":true,\"selfMonitor\":{\"enabled\":false}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"jobLabel\":\"component\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[{\"action\":\"drop\",\"regex\":\"apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\",\"sourceLabels\":[\"__name__\",\"le\"]}],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}},\"targetLimit\":0},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLimit\":0}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"additionalLabels\":{},\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLimit\":0}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":2381,\"targetPort\":2381},\"serviceMonitor\":{\"additionalLabels\":{},\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"keyFile\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"http\",\"selector\":{},\"serverName\":\"\",\"targetLimit\":0}},\"kubeProxy\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":10249,\"targetPort\":10249},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLimit\":0}},\"kubeScheduler\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLimit\":0}},\"kubeStateMetrics\":{\"enabled\":true},\"kubeTargetVersionOverride\":\"\",\"kubeVersionOverride\":\"\",\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"additionalLabels\":{},\"attachMetadata\":{\"node\":false},\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[{\"action\":\"drop\",\"regex\":\"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_memory_(mapped_file|swap)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_(file_descriptors|tasks_state|threads_max)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_spec.*\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\".+;\",\"sourceLabels\":[\"id\",\"pod\"]}],\"cAdvisorRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"honorLabels\":true,\"honorTimestamps\":true,\"https\":true,\"insecureSkipVerify\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"sampleLimit\":0,\"targetLimit\":0}},\"kubernetesServiceMonitors\":{\"enabled\":true},\"nameOverride\":\"prom-was\",\"namespaceOverride\":\"\",\"nodeExporter\":{\"enabled\":true,\"forceDeployDashboards\":false,\"operatingSystems\":{\"darwin\":{\"enabled\":true},\"linux\":{\"enabled\":true}}},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"agentMode\":false,\"annotations\":{},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{\"nginx.ingress.kubernetes.io/rewrite-target\":\"/$2\"},\"enabled\":true,\"hosts\":[],\"ingressClassName\":\"nginx\",\"labels\":{},\"paths\":[\"/prom(/|$)(.*)\"],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalAlertRelabelConfigsSecret\":{},\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"automountServiceAccountToken\":true,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enableRemoteWriteReceiver\":false,\"enforcedKeepDroppedTargets\":0,\"enforcedLabelLimit\":false,\"enforcedLabelNameLengthLimit\":false,\"enforcedLabelValueLengthLimit\":false,\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"enforcedTargetLimit\":false,\"evaluationInterval\":\"\",\"excludedFromEnforcement\":[],\"exemplars\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"hostAliases\":[],\"hostNetwork\":false,\"ignoreNamespaceSelectors\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.53.1\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"maximumStartupDurationSeconds\":0,\"minReadySeconds\":0,\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"persistentVolumeClaimRetentionPolicy\":{},\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorNamespaceSelector\":{},\"podMonitorSelector\":{},\"podMonitorSelectorNilUsesHelmValues\":true,\"portName\":\"http-web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{},\"probeSelectorNilUsesHelmValues\":true,\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"sampleLimit\":false,\"scrapeClasses\":[],\"scrapeConfigNamespaceSelector\":{},\"scrapeConfigSelector\":{},\"scrapeConfigSelectorNilUsesHelmValues\":true,\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{},\"serviceMonitorSelectorNilUsesHelmValues\":true,\"shards\":1,\"storageSpec\":{},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"tracingConfig\":{},\"tsdb\":{\"outOfOrderTimeWindow\":\"0s\"},\"version\":\"\",\"volumeMounts\":[],\"volumes\":[],\"walCompression\":true,\"web\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"publishNotReadyAddresses\":false,\"reloaderWebPort\":8080,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9090,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"None\",\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"LoadBalancer\"},\"thanosServiceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"tlsConfig\":{}}},\"prometheus-node-exporter\":{\"extraArgs\":[\"--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\",\"--collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"],\"namespaceOverride\":\"\",\"podLabels\":{\"jobLabel\":\"node-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"pspEnabled\":false},\"releaseLabel\":true,\"service\":{\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"portName\":\"http-metrics\"}},\"prometheus-windows-exporter\":{\"config\":\"collectors:\\n  enabled: '[defaults],memory,container'\",\"podLabels\":{\"jobLabel\":\"windows-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"jobLabel\":\"jobLabel\"}},\"releaseLabel\":true},\"prometheusOperator\":{\"admissionWebhooks\":{\"annotations\":{},\"caBundle\":\"\",\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"createSecretJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"deployment\":{\"affinity\":{},\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"dnsConfig\":{},\"enabled\":false,\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/admission-webhook\",\"sha\":\"\",\"tag\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{},\"podLabels\":{},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":10,\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":31080,\"nodePortTls\":31443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"automountServiceAccountToken\":false,\"create\":true,\"name\":\"\"},\"strategy\":{},\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[]},\"enabled\":true,\"failurePolicy\":\"\",\"namespaceSelector\":{},\"objectSelector\":{},\"patch\":{\"affinity\":{},\"annotations\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"registry.k8s.io\",\"repository\":\"ingress-nginx/kube-webhook-certgen\",\"sha\":\"\",\"tag\":\"v20221220-controller-v1.5.1-58-g787ea74b6\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceAccount\":{\"automountServiceAccountToken\":true,\"create\":true},\"tolerations\":[],\"ttlSecondsAfterFinished\":60},\"patchWebhookJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"timeoutSeconds\":10},\"affinity\":{},\"alertmanagerConfigNamespaces\":[],\"alertmanagerInstanceNamespaces\":[],\"alertmanagerInstanceSelector\":\"\",\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"denyNamespaces\":[],\"dnsConfig\":{},\"enabled\":true,\"env\":{\"GOGC\":\"30\"},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-operator\",\"sha\":\"\",\"tag\":\"\"},\"kubeletService\":{\"enabled\":true,\"name\":\"\",\"namespace\":\"kube-system\",\"selector\":\"\"},\"labels\":{},\"namespaces\":{},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"prometheusConfigReloader\":{\"enableProbe\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-config-reloader\",\"sha\":\"\",\"tag\":\"\"},\"resources\":{}},\"prometheusInstanceNamespaces\":[],\"prometheusInstanceSelector\":\"\",\"resources\":{},\"revisionHistoryLimit\":10,\"secretFieldSelector\":\"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30080,\"nodePortTls\":30443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"selfMonitor\":true,\"targetLimit\":0},\"strategy\":{},\"thanosImage\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.35.1\"},\"thanosRulerInstanceNamespaces\":[],\"thanosRulerInstanceSelector\":\"\",\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[],\"verticalPodAutoscaler\":{\"controlledResources\":[],\"enabled\":false,\"maxAllowed\":{},\"minAllowed\":{},\"updatePolicy\":{\"updateMode\":\"Auto\"}}},\"thanosRuler\":{\"annotations\":{},\"enabled\":false,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30905,\"port\":10902,\"targetPort\":10902,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"thanosRulerSpec\":{\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"affinity\":{},\"alertDropLabels\":[],\"alertmanagersConfig\":{\"existingSecret\":{},\"secret\":{}},\"containers\":[],\"evaluationInterval\":\"\",\"externalPrefix\":null,\"externalPrefixNilUsesHelmValues\":true,\"image\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.35.1\"},\"initContainers\":[],\"labels\":{},\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"objectStorageConfig\":{\"existingSecret\":{},\"secret\":{}},\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"web\",\"priorityClassName\":\"\",\"queryConfig\":{\"existingSecret\":{},\"secret\":{}},\"queryEndpoints\":[],\"replicas\":1,\"resources\":{},\"retention\":\"24h\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]}},\"windowsMonitoring\":{\"enabled\":false}}","version":"61.3.2"}],"name":"prometheus","namespace":"prometheus","pass_credentials":false,"postrender":[],"recreate_pods":true,"render_subchart_notes":true,"replace":false,"repository":"./helm/","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":[{"name":"grafana.enabled","type":"","value":"false"},{"name":"grafana.grafana\\.ini.server.domain","type":"","value":"192.168.56.101:30232"},{"name":"grafana.grafana\\.ini.server.root_url","type":"","value":"http://192.168.58.101:31545/grafana"},{"name":"grafana.grafana\\.ini.server.serve_from_sub_path","type":"","value":"true"},{"name":"grafana.ingress.enabled","type":"","value":"false"},{"name":"grafana.ingress.ingressClassName","type":"","value":"nginx"},{"name":"grafana.ingress.path","type":"","value":"/grafana"},{"name":"nameOverride","type":"","value":"prom-was"},{"name":"prometheus.ingress.annotations.nginx\\.ingress\\.kubernetes\\.io/rewrite-target","type":"","value":"/$2"},{"name":"prometheus.ingress.enabled","type":"","value":"true"},{"name":"prometheus.ingress.ingressClassName","type":"","value":"nginx"},{"name":"prometheus.ingress.paths[0]","type":"","value":"/prom(/|$)(.*)"}],"set_list":[],"set_sensitive":[],"skip_crds":false,"status":"deployed","timeout":300,"values":["# Default values for kube-prometheus-stack.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of kube-prometheus-stack for `app:` labels\n##\nnameOverride: \"\"\n\n## Override the deployment namespace\n##\nnamespaceOverride: \"\"\n\n## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6\n##\nkubeTargetVersionOverride: \"\"\n\n## Allow kubeVersion to be overridden while creating the ingress\n##\nkubeVersionOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Install Prometheus Operator CRDs\n##\ncrds:\n  enabled: true\n\n## custom Rules to override \"for\" and \"severity\" in defaultRules\n##\ncustomRules: {}\n  # AlertmanagerFailedReload:\n  #   for: 3m\n  # AlertmanagerMembersInconsistent:\n  #   for: 5m\n  #   severity: \"warning\"\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    configReloaders: true\n    general: true\n    k8sContainerCpuUsageSecondsTotal: true\n    k8sContainerMemoryCache: true\n    k8sContainerMemoryRss: true\n    k8sContainerMemorySwap: true\n    k8sContainerResource: true\n    k8sContainerMemoryWorkingSetBytes: true\n    k8sPodOwner: true\n    kubeApiserverAvailability: true\n    kubeApiserverBurnrate: true\n    kubeApiserverHistogram: true\n    kubeApiserverSlos: true\n    kubeControllerManager: true\n    kubelet: true\n    kubeProxy: true\n    kubePrometheusGeneral: true\n    kubePrometheusNodeRecording: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    kubeSchedulerAlerting: true\n    kubeSchedulerRecording: true\n    kubeStateMetrics: true\n    network: true\n    node: true\n    nodeExporterAlerting: true\n    nodeExporterRecording: true\n    prometheus: true\n    prometheusOperator: true\n    windows: true\n\n  ## Reduce app namespace alert scope\n  appNamespacesTarget: \".*\"\n\n  ## Set keep_firing_for for all alerts\n  keepFiringFor: \"\"\n\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n  ## Additional labels for PrometheusRule alerts\n  additionalRuleLabels: {}\n\n  ## Additional annotations for PrometheusRule alerts\n  additionalRuleAnnotations: {}\n\n  ## Additional labels for specific PrometheusRule alert groups\n  additionalRuleGroupLabels:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8sContainerCpuUsageSecondsTotal: {}\n    k8sContainerMemoryCache: {}\n    k8sContainerMemoryRss: {}\n    k8sContainerMemorySwap: {}\n    k8sContainerResource: {}\n    k8sPodOwner: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  ## Additional annotations for specific PrometheusRule alerts groups\n  additionalRuleGroupAnnotations:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8sContainerCpuUsageSecondsTotal: {}\n    k8sContainerMemoryCache: {}\n    k8sContainerMemoryRss: {}\n    k8sContainerMemorySwap: {}\n    k8sContainerResource: {}\n    k8sPodOwner: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  additionalAggregationLabels: []\n\n  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.\n  runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\"\n\n  ## Disabled PrometheusRule alerts\n  disabled: {}\n  # KubeAPIDown: true\n  # NodeRAIDDegraded: true\n\n## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.\n##\n# additionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRulesMap: {}\n#  rule-name:\n#    groups:\n#    - name: my_group\n#      rules:\n#      - record: my_record\n#        expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n\n    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs\n    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\n    createAggregateClusterRoles: false\n    pspEnabled: false\n    pspAnnotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)\n  ##\n  imageRegistry: \"\"\n\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n  # or\n  # - \"image-pull-secret\"\n\nwindowsMonitoring:\n  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')\n  enabled: false\n\n## Configuration for prometheus-windows-exporter\n## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter\n##\nprometheus-windows-exporter:\n  ## Enable ServiceMonitor and set Kubernetes label to use as a job label\n  ##\n  prometheus:\n    monitor:\n      enabled: true\n      jobLabel: jobLabel\n\n  releaseLabel: true\n\n  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards\n  ##\n  podLabels:\n    jobLabel: windows-exporter\n\n  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards\n  ##\n  config: |-\n    collectors:\n      enabled: '[defaults],memory,container'\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n\n  ## Deploy alertmanager\n  ##\n  enabled: true\n\n  ## Annotations for Alertmanager\n  ##\n  annotations: {}\n\n  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2\n  ##\n  apiVersion: v2\n\n  ## @param alertmanager.enableFeatures Enable access to Alertmanager disabled features.\n  ##\n  enableFeatures: []\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    inhibit_rules:\n      - source_matchers:\n          - 'severity = critical'\n        target_matchers:\n          - 'severity =~ warning|info'\n        equal:\n          - 'namespace'\n          - 'alertname'\n      - source_matchers:\n          - 'severity = warning'\n        target_matchers:\n          - 'severity = info'\n        equal:\n          - 'namespace'\n          - 'alertname'\n      - source_matchers:\n          - 'alertname = InfoInhibitor'\n        target_matchers:\n          - 'severity = info'\n        equal:\n          - 'namespace'\n      - target_matchers:\n          - 'alertname = InfoInhibitor'\n    route:\n      group_by: ['namespace']\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: 'null'\n      routes:\n      - receiver: 'null'\n        matchers:\n          - alertname = \"Watchdog\"\n    receivers:\n    - name: 'null'\n    templates:\n    - '/etc/alertmanager/config/*.tmpl'\n\n  ## Alertmanager configuration directives (as string type, preferred over the config hash map)\n  ## stringConfig will be used only, if tplConfig is true\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  stringConfig: \"\"\n\n  ## Pass the Alertmanager configuration directives through Helm's templating\n  ## engine. If the Alertmanager configuration contains Alertmanager templates,\n  ## they'll need to be properly escaped so that they are not interpreted by\n  ## Helm\n  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function\n  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string\n  ##      https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  tplConfig: false\n\n  ## Alertmanager template files to format alerts\n  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if\n  ## they have a .tmpl file suffix will be loaded. See config.templates above\n  ## to change, add other suffixes. If adding other suffixes, be sure to update\n  ## config.templates above to include those suffixes.\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  ## An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:* {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n  #       {{ end }}\n  #       {{ end }}\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Override ingress to a different defined port on the service\n    # servicePort: 8081\n    ## Override ingress to a different service then the default, this is useful if you need to\n    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)\n    # serviceName: kube-prometheus-stack-alertmanager-0\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  ## Configuration for Alertmanager secret\n  ##\n  secret:\n    annotations: {}\n\n  ## Configuration for creating an Ingress that will map to each Alertmanager replica service\n  ## alertmanager.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for alertmanager per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"alertmanager\"\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for Alertmanager Service to listen on\n    ##\n    port: 9093\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 9093\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30903\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for Alertmanager service\n    ##\n    additionalPorts: []\n    # - name: oauth-proxy\n    #   port: 8081\n    #   targetPort: 8081\n    # - name: oauth-metrics\n    #   port: 8082\n    #   targetPort: 8082\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or 'None'\n    ##\n    sessionAffinity: None\n\n    ## If you want to modify the ClientIP sessionAffinity timeout\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\n    ##\n    sessionAffinityConfig:\n      clientIP:\n        timeoutSeconds: 10800\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a separate Service for each statefulset Alertmanager replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Alertmanager Service per replica to listen on\n    ##\n    port: 9093\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9093\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30904\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a ServiceMonitor for AlertManager\n  ##\n  serviceMonitor:\n    ## If true, a ServiceMonitor will be created for the AlertManager service.\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## enableHttp2: Whether to enable HTTP2.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n    enableHttp2: true\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/alertmanager\n      tag: v0.27.0\n      sha: \"\"\n\n    ## If true then the user will be responsible to provide a secret with alertmanager configuration\n    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used\n    ##\n    useExistingSecret: false\n\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## If false then the user will opt out of automounting API credentials.\n    ##\n    automountServiceAccountToken: true\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for\n    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.\n    ##\n    # configSecret:\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec\n    web: {}\n\n    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.\n    ##\n    alertmanagerConfigSelector: {}\n    ## Example which selects all alertmanagerConfig resources\n    ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\"\n    # alertmanagerConfigSelector:\n    #   matchExpressions:\n    #     - key: alertconfig\n    #       operator: In\n    #       values:\n    #         - example-config\n    #         - example-config-2\n    #\n    ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\"\n    # alertmanagerConfigSelector:\n    #   matchLabels:\n    #     role: example-config\n\n    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.\n    ##\n    alertmanagerConfigNamespaceSelector: {}\n    ## Example which selects all namespaces\n    ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchExpressions:\n    #     - key: alertmanagerconfig\n    #       operator: In\n    #       values:\n    #         - example-namespace\n    #         - example-namespace-2\n\n    ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchLabels:\n    #     alertmanagerconfig: enabled\n\n    ## AlermanagerConfig to be used as top level configuration\n    ##\n    alertmanagerConfiguration: {}\n    ## Example with select a global alertmanagerconfig\n    # alertmanagerConfiguration:\n    #   name: global-alertmanager-Configuration\n\n    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:\n    ##\n    alertmanagerConfigMatcherStrategy: {}\n    ## Example with use OnNamespace strategy\n    # alertmanagerConfigMatcherStrategy:\n    #   type: OnNamespace\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #     selector: {}\n\n\n    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false\n    ##\n    externalUrl:\n\n    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the alertmanager instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: alertmanager\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\n    #   args:\n    #   - --upstream=http://127.0.0.1:9093\n    #   - --http-address=0.0.0.0:8081\n    #   - --metrics-address=0.0.0.0:8082\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   - containerPort: 8082\n    #     name: oauth-metrics\n    #     protocol: TCP\n    #   resources: {}\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n    ## PortName to use for Alert Manager.\n    ##\n    portName: \"http-web\"\n\n    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918\n    ##\n    clusterAdvertiseAddress: false\n\n    ## clusterGossipInterval determines interval between gossip attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Gos time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterGossipInterval: \"\"\n\n    ## clusterPeerTimeout determines timeout for cluster peering.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Gos time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPeerTimeout: \"\"\n\n    ## clusterPushpullInterval determines interval between pushpull attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Gos time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPushpullInterval: \"\"\n\n    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.\n    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.\n    forceEnableClusterMode: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\n    ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)\n    additionalConfigString: \"\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\n##\ngrafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  ## Editable flag for the default dashboards\n  ##\n  defaultDashboardsEditable: true\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n#  grafana.ini:\n#    server:\n#      #domain: \"192.168.56.101:30343\"\n#      #root_url: \"http://192.168.56.101:30343/grafana\"\n#      #serve_from_sub_path: true\n#      domain: \"\"\n#      root_url: \"\"\n#      serve_from_sub_path: false\n\n\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  # # To make Grafana persistent (Using Statefulset)\n  # #\n  # persistence:\n  #   enabled: true\n  #   type: sts\n  #   storageClassName: \"storageClassName\"\n  #   accessModes:\n  #     - ReadWriteOnce\n  #   size: 20Gi\n  #   finalizers:\n  #     - kubernetes.io/pvc-protection\n\n  serviceAccount:\n    create: true\n    autoMount: true\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n      # Allow discovery in all namespaces for dashboards\n      searchNamespace: ALL\n\n      # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels\n      enableNewTablePanelSyntax: false\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n      isDefaultDatasource: true\n\n      name: Prometheus\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      ## Prometheus request timeout in seconds\n      # timeout: 30\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Set method for HTTP to send query to datasource\n      httpMethod: POST\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations: {}\n        # datasourceUid: Jaeger\n        # traceIdLabelName: trace_id\n      alertmanager:\n        enabled: true\n        name: Alertmanager\n        uid: alertmanager\n        handleGrafanaManagedAlerts: false\n        implementation: prometheus\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090\n  #   version: 1\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n    ipFamilies: []\n    ipFamilyPolicy: \"\"\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Flag to disable all the kubernetes component scrapers\n##\nkubernetesServiceMonitors:\n  enabled: true\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.\n    ##\n    attachMetadata:\n      node: false\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## If true, Prometheus use (respect) labels provided by exporter.\n    ##\n    honorLabels: true\n\n    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.\n    ##\n    honorTimestamps: true\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Skip TLS certificate validation when scraping.\n    ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs\n    ##\n    insecureSkipVerify: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_memory_(mapped_file|swap)'\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_(file_descriptors|tasks_state|threads_max)'\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_spec.*'\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: '.+;'\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: kube-controller-manager\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    enabled: true\n    port: 9153\n    targetPort: 9153\n\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-dns\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-dns\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2381\n    targetPort: 2381\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: etcd\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: true\n\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.23.\n    ##\n    port: null\n    targetPort: null\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   component: kube-scheduler\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    ## Enable scraping kube-scheduler over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    component: kube-scheduler\n\n    ## Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    ## Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kube proxy\n##\nkubeProxy:\n  enabled: true\n\n  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  service:\n    enabled: true\n    port: 10249\n    targetPort: 10249\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n    # selector:\n    #   k8s-app: kube-proxy\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## port: Name of the port the metrics will be scraped from\n    ##\n    port: http-metrics\n\n    jobLabel: jobLabel\n    selector: {}\n    #  matchLabels:\n    #    k8s-app: kube-proxy\n\n    ## Enable scraping kube-proxy over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  namespaceOverride: \"\"\n  rbac:\n    create: true\n  releaseLabel: true\n  prometheus:\n    monitor:\n      enabled: true\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      # Keep labels from scraped data, overriding server-side labels\n      ##\n      honorLabels: true\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - action: keep\n      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n      #   sourceLabels: [__name__]\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n\n  selfMonitor:\n    enabled: false\n\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n  operatingSystems:\n    linux:\n      enabled: true\n    darwin:\n      enabled: true\n\n  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  namespaceOverride: \"\"\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  releaseLabel: true\n  extraArgs:\n    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\n  service:\n    portName: http-metrics\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n  prometheus:\n    monitor:\n      enabled: true\n\n      jobLabel: jobLabel\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - sourceLabels: [__name__]\n      #   separator: ;\n      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n      #   replacement: $1\n      #   action: drop\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n  rbac:\n    ## If true, create PSPs for node-exporter\n    ##\n    pspEnabled: false\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n\n  ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-operator' by default\n  fullnameOverride: \"\"\n\n  ## Number of old replicasets to retain ##\n  ## The default value is 10, 0 will garbage-collect old replicasets ##\n  revisionHistoryLimit: 10\n\n  ## Strategy of the deployment\n  ##\n  strategy: {}\n\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\n  ##\n  tls:\n    enabled: true\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n    tlsMinVersion: VersionTLS13\n    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n    internalPort: 10250\n\n  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted\n  ## rules from making their way into prometheus and potentially preventing the container from starting\n  admissionWebhooks:\n    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly\n    ## IgnoreOnInstallOnly - If Release.IsInstall returns \"true\", set \"Ignore\" otherwise \"Fail\"\n    failurePolicy: \"\"\n    ## The default timeoutSeconds is 10 and the maximum value is 30.\n    timeoutSeconds: 10\n    enabled: true\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\n    ## If unspecified, system trust roots on the apiserver are used.\n    caBundle: \"\"\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\n    ## certs ahead of time if you wish.\n    ##\n    annotations: {}\n    #   argocd.argoproj.io/hook: PreSync\n    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n\n    namespaceSelector: {}\n    objectSelector: {}\n\n\n    deployment:\n      enabled: false\n\n      ## Number of replicas\n      ##\n      replicas: 1\n\n      ## Strategy of the deployment\n      ##\n      strategy: {}\n\n      # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n      podDisruptionBudget: {}\n        # maxUnavailable: 1\n        # minAvailable: 1\n\n      ## Number of old replicasets to retain ##\n      ## The default value is 10, 0 will garbage-collect old replicasets ##\n      revisionHistoryLimit: 10\n\n      ## Prometheus-Operator v0.39.0 and later support TLS natively.\n      ##\n      tls:\n        enabled: true\n        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n        tlsMinVersion: VersionTLS13\n        # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n        internalPort: 10250\n\n      ## Service account for Prometheus Operator Webhook to use.\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n      ##\n      serviceAccount:\n        automountServiceAccountToken: false\n        create: true\n        name: \"\"\n\n      ## Configuration for Prometheus operator Webhook service\n      ##\n      service:\n        annotations: {}\n        labels: {}\n        clusterIP: \"\"\n        ipDualStack:\n          enabled: false\n          ipFamilies: [\"IPv6\", \"IPv4\"]\n          ipFamilyPolicy: \"PreferDualStack\"\n\n        ## Port to expose on each node\n        ## Only used if service.type is 'NodePort'\n        ##\n        nodePort: 31080\n\n        nodePortTls: 31443\n\n        ## Additional ports to open for Prometheus operator Webhook service\n        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n        ##\n        additionalPorts: []\n\n        ## Loadbalancer IP\n        ## Only use if service.type is \"LoadBalancer\"\n        ##\n        loadBalancerIP: \"\"\n        loadBalancerSourceRanges: []\n\n        ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n        ##\n        externalTrafficPolicy: Cluster\n\n        ## Service type\n        ## NodePort, ClusterIP, LoadBalancer\n        ##\n        type: ClusterIP\n\n        ## List of IP addresses at which the Prometheus server service is available\n        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n        ##\n        externalIPs: []\n\n      # ## Labels to add to the operator webhook deployment\n      # ##\n      labels: {}\n\n      ## Annotations to add to the operator webhook deployment\n      ##\n      annotations: {}\n\n      ## Labels to add to the operator webhook pod\n      ##\n      podLabels: {}\n\n      ## Annotations to add to the operator webhook pod\n      ##\n      podAnnotations: {}\n\n      ## Assign a PriorityClassName to pods if set\n      # priorityClassName: \"\"\n\n      ## Define Log Format\n      # Use logfmt (default) or json logging\n      # logFormat: logfmt\n\n      ## Decrease log verbosity to errors only\n      # logLevel: error\n\n      ## Prometheus-operator webhook image\n      ##\n      image:\n        registry: quay.io\n        repository: prometheus-operator/admission-webhook\n        # if not set appVersion field from Chart.yaml is used\n        tag: \"\"\n        sha: \"\"\n        pullPolicy: IfNotPresent\n\n      ## Define Log Format\n      # Use logfmt (default) or json logging\n      # logFormat: logfmt\n\n      ## Decrease log verbosity to errors only\n      # logLevel: error\n\n\n      ## Liveness probe\n      ##\n      livenessProbe:\n        enabled: true\n        failureThreshold: 3\n        initialDelaySeconds: 30\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n\n      ## Readiness probe\n      ##\n      readinessProbe:\n        enabled: true\n        failureThreshold: 3\n        initialDelaySeconds: 5\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n\n      ## Resource limits \u0026 requests\n      ##\n      resources: {}\n      # limits:\n      #   cpu: 200m\n      #   memory: 200Mi\n      # requests:\n      #   cpu: 100m\n      #   memory: 100Mi\n\n      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n      ##\n      hostNetwork: false\n\n      ## Define which Nodes the Pods are scheduled on.\n      ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n      ##\n      nodeSelector: {}\n\n      ## Tolerations for use with node taints\n      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n      ##\n      tolerations: []\n      # - key: \"key\"\n      #   operator: \"Equal\"\n      #   value: \"value\"\n      #   effect: \"NoSchedule\"\n\n      ## Assign custom affinity rules to the prometheus operator\n      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n      ##\n      affinity: {}\n        # nodeAffinity:\n        #   requiredDuringSchedulingIgnoredDuringExecution:\n        #     nodeSelectorTerms:\n        #     - matchExpressions:\n        #       - key: kubernetes.io/e2e-az-name\n        #         operator: In\n        #         values:\n        #         - e2e-az1\n      #         - e2e-az2\n      dnsConfig: {}\n        # nameservers:\n        #   - 1.2.3.4\n        # searches:\n        #   - ns1.svc.cluster-domain.example\n        #   - my.dns.search.suffix\n        # options:\n        #   - name: ndots\n        #     value: \"2\"\n        #   - name: edns0\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n\n      ## Container-specific security context configuration\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      containerSecurityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n            - ALL\n\n      ## If false then the user will opt out of automounting API credentials.\n      ##\n      automountServiceAccountToken: true\n\n    patch:\n      enabled: true\n      image:\n        registry: registry.k8s.io\n        repository: ingress-nginx/kube-webhook-certgen\n        tag: v20221220-controller-v1.5.1-58-g787ea74b6\n        sha: \"\"\n        pullPolicy: IfNotPresent\n      resources: {}\n      ## Provide a priority class name to the webhook patching job\n      ##\n      priorityClassName: \"\"\n      ttlSecondsAfterFinished: 60\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n      podAnnotations: {}\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n\n      ## SecurityContext holds pod-level security attributes and common container settings.\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n      ## Service account for Prometheus Operator Webhook Job Patch to use.\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n      ##\n      serviceAccount:\n        create: true\n        automountServiceAccountToken: true\n\n    # Security context for create job container\n    createSecretJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n\n      # Security context for patch job container\n    patchWebhookJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false\n      # self-signed root certificate\n      rootCert:\n        duration: \"\"  # default to be 5y\n      admissionCert:\n        duration: \"\"  # default to be 1y\n      # issuerRef:\n      #   name: \"issuer\"\n      #   kind: \"ClusterIssuer\"\n\n  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).\n  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration\n  ##\n  namespaces: {}\n    # releaseNamespace: true\n    # additional:\n    # - kube-system\n\n  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).\n  ##\n  denyNamespaces: []\n\n  ## Filter namespaces to look for prometheus-operator custom resources\n  ##\n  alertmanagerInstanceNamespaces: []\n  alertmanagerConfigNamespaces: []\n  prometheusInstanceNamespaces: []\n  thanosRulerInstanceNamespaces: []\n\n  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.\n  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)\n  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094\n  ##\n  # clusterDomain: \"cluster.local\"\n\n  networkPolicy:\n    ## Enable creation of NetworkPolicy resources.\n    ##\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   egress:\n\n    ## match labels used in selector\n    # matchLabels: {}\n\n  ## Service account for Prometheus Operator to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    automountServiceAccountToken: true\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30080\n\n    nodePortTls: 30443\n\n  ## Additional ports to open for Prometheus operator service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n  ##\n    additionalPorts: []\n\n  ## Loadbalancer IP\n  ## Only use if service.type is \"LoadBalancer\"\n  ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n  ## Service type\n  ## NodePort, ClusterIP, LoadBalancer\n  ##\n    type: ClusterIP\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  # ## Labels to add to the operator deployment\n  # ##\n  labels: {}\n\n  ## Annotations to add to the operator deployment\n  ##\n  annotations: {}\n\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  kubeletService:\n    ## If true, the operator will create and maintain a service for scraping kubelets\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md\n    ##\n    enabled: true\n    namespace: kube-system\n    selector: \"\"\n    ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default\n    name: \"\"\n\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## If true, create a serviceMonitor for prometheus operator\n    ##\n    selfMonitor: true\n\n    ## Labels for ServiceMonitor\n    additionalLabels: {}\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.\n    scrapeTimeout: \"\"\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  ## Operator Environment\n  ##  env:\n  ##    VARIABLE: value\n  env:\n    GOGC: \"30\"\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign custom affinity rules to the prometheus operator\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n    seccompProfile:\n      type: RuntimeDefault\n\n  ## Container-specific security context configuration\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ##\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n      - ALL\n\n  # Enable vertical pod autoscaler support for prometheus-operator\n  verticalPodAutoscaler:\n    enabled: false\n\n    # Recommender responsible for generating recommendation for the object.\n    # List should be empty (then the default recommender will generate the recommendation)\n    # or contain exactly one recommender.\n    # recommenders:\n    # - name: custom-recommender-performance\n\n    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory\n    controlledResources: []\n    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.\n    # controlledValues: RequestsAndLimits\n\n    # Define the max allowed resources for the pod\n    maxAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n    # Define the min allowed resources for the pod\n    minAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n\n    updatePolicy:\n      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction\n      # minReplicas: 1\n      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates\n      # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\".\n      updateMode: Auto\n\n  ## Prometheus-operator image\n  ##\n  image:\n    registry: quay.io\n    repository: prometheus-operator/prometheus-operator\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    sha: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus image to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImage: prometheus/prometheus\n\n  ## Prometheus image registry to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImageRegistry: quay.io\n\n  ## Alertmanager image to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImage: prometheus/alertmanager\n\n  ## Alertmanager image registry to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImageRegistry: quay.io\n\n  ## Prometheus-config-reloader\n  ##\n  prometheusConfigReloader:\n    image:\n      registry: quay.io\n      repository: prometheus-operator/prometheus-config-reloader\n      # if not set appVersion field from Chart.yaml is used\n      tag: \"\"\n      sha: \"\"\n\n    # add prometheus config reloader liveness and readiness probe. Default: false\n    enableProbe: false\n\n    # resource config for prometheusConfigReloader\n    resources: {}\n      # requests:\n      #   cpu: 200m\n      #   memory: 50Mi\n      # limits:\n      #   cpu: 200m\n      #   memory: 50Mi\n\n  ## Thanos side-car image when configured\n  ##\n  thanosImage:\n    registry: quay.io\n    repository: thanos/thanos\n    tag: v0.35.1\n    sha: \"\"\n\n  ## Set a Label Selector to filter watched prometheus and prometheusAgent\n  ##\n  prometheusInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched alertmanager\n  ##\n  alertmanagerInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched thanosRuler\n  thanosRulerInstanceSelector: \"\"\n\n  ## Set a Field Selector to filter watched secrets\n  ##\n  secretFieldSelector: \"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\"\n\n  ## If false then the user will opt out of automounting API credentials.\n  ##\n  automountServiceAccountToken: true\n\n  ## Additional volumes\n  ##\n  extraVolumes: []\n\n  ## Additional volume mounts\n  ##\n  extraVolumeMounts: []\n\n## Deploy a Prometheus instance\n##\nprometheus:\n  enabled: true\n\n  ## Toggle prometheus into agent mode\n  ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md\n  ##\n  agentMode: false\n\n  ## Annotations for Prometheus\n  ##\n  annotations: {}\n\n  ## Configure network policy for the prometheus\n  networkPolicy:\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   endpointSelector:\n    #   egress:\n    #   ingress:\n\n    # egress:\n    # - {}\n    # ingress:\n    # - {}\n    # podSelector:\n    #   matchLabels:\n    #     app: prometheus\n\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  # Service for thanos service discovery on sidecar\n  # Enable this can make Thanos Query can use\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\n  # Thanos sidecar on prometheus nodes\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\n  thanosService:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Service dual stack\n    ##\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## ClusterIP to assign\n    # Default is to make this a headless service (\"None\")\n    clusterIP: \"None\"\n\n    ## Port to expose on each node, if service type is NodePort\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  # ServiceMonitor to scrape Sidecar metrics\n  # Needs thanosService to be enabled as well\n  thanosServiceMonitor:\n    enabled: false\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n\n    ## relabel configs to apply to samples before ingestion.\n    relabelings: []\n\n  # Service for external access to sidecar\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\n  thanosServiceExternal:\n    enabled: false\n    annotations: {}\n    labels: {}\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: LoadBalancer\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for Prometheus Service to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port for Prometheus Reloader to listen on\n    ##\n    reloaderWebPort: 8080\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Additional ports to open for Prometheus service\n    ##\n    additionalPorts: []\n    # additionalPorts:\n    # - name: oauth-proxy\n    #   port: 8081\n    #   targetPort: 8081\n    # - name: oauth-metrics\n    #   port: 8082\n    #   targetPort: 8082\n\n    ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not\n    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec\n    publishNotReadyAddresses: false\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or 'None'\n    ##\n    sessionAffinity: None\n\n    ## If you want to modify the ClientIP sessionAffinity timeout\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\n    ##\n    sessionAffinityConfig:\n      clientIP:\n        timeoutSeconds: 10800\n\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Prometheus Service per replica to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30091\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Service dual stack\n    ##\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  # Ingress exposes thanos sidecar outside the cluster\n  thanosIngress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n    servicePort: 10901\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30901\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanos-gateway.domain.com\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Thanos Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanos-gateway-tls\n    #   hosts:\n    #   - thanos-gateway.domain.com\n    #\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Redirect ingress to an additional defined port on the service\n    # servicePort: 8081\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\n  ## prometheus.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"prometheus\"\n\n  ## Configure additional options for default pod security policy for Prometheus\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  podSecurityPolicy:\n    allowedCapabilities: []\n    allowedHostPaths: []\n    volumes: []\n\n  serviceMonitor:\n    ## If true, create a serviceMonitor for prometheus\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## Statefulset's persistent volume claim retention policy\n    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether\n    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down\n    ## and deleting statefulset, respectively. Requires 1.27.0+.\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    persistentVolumeClaimRetentionPolicy: {}\n    #  whenDeleted: Retain\n    #  whenScaled: Retain\n\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\n    ##\n    ## AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in the pod,\n    ## If the field isnt set, the operator mounts the service account token by default.\n    ## Warning: be aware that by default, Prometheus requires the service account token for Kubernetes service discovery,\n    ## It is possible to use strategic merge patch to project the service account token into the prometheus container.\n    automountServiceAccountToken: true\n\n    disableCompaction: false\n    ## APIServerConfig\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig\n    ##\n    apiserverConfig: {}\n\n    ## Allows setting additional arguments for the Prometheus container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus\n    additionalArgs: []\n\n    ## Interval between consecutive scrapes.\n    ## Defaults to 30s.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\n    ##\n    scrapeInterval: \"\"\n\n    ## Number of seconds to wait for target to respond before erroring\n    ##\n    scrapeTimeout: \"\"\n\n    ## List of scrape classes to expose to scraping objects such as\n    ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.\n    ##\n    scrapeClasses: []\n    # - name: istio-mtls\n    #   default: false\n    #   tlsConfig:\n    #     caFile: /etc/prometheus/secrets/istio.default/root-cert.pem\n    #     certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\n    ## This is disabled by default.\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\n    ##\n    enableAdminAPI: false\n\n    ## Sets version of Prometheus overriding the Prometheus version as derived\n    ## from the image tag. Useful in cases where the tag does not follow semver v2.\n    version: \"\"\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig\n    web: {}\n\n    ## Exemplars related settings that are runtime reloadable.\n    ## It requires to enable the exemplar storage feature to be effective.\n    exemplars: \"\"\n      ## Maximum number of exemplars stored in memory for all series.\n      ## If not set, Prometheus uses its default value.\n      ## A value of zero or less than zero disables the storage.\n      # maxSize: 100000\n\n    # EnableFeatures API enables access to Prometheus disabled features.\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\n    enableFeatures: []\n    # - exemplar-storage\n\n    ## Image of Prometheus.\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/prometheus\n      tag: v2.53.1\n      sha: \"\"\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: prometheus\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n    #   pathPrefix: \"\"\n    #   tlsConfig: {}\n    #   bearerTokenFile: \"\"\n    #   apiVersion: v2\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## enable --web.enable-remote-write-receiver flag on prometheus-server\n    ##\n    enableRemoteWriteReceiver: false\n\n    ## Name of the external label used to denote replica name\n    ##\n    replicaExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote replica name\n    ##\n    replicaExternalLabelNameClear: false\n\n    ## Name of the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelNameClear: false\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.\n    ruleNamespaceSelector: {}\n    ## Example which selects PrometheusRules in namespaces with label \"prometheus\" set to \"somelabel\"\n    # ruleNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ##\n    serviceMonitorNamespaceSelector: {}\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the podmonitors created\n    ##\n    podMonitorSelectorNilUsesHelmValues: true\n\n    ## PodMonitors to be selected for target discovery.\n    ## If {}, select all PodMonitors\n    ##\n    podMonitorSelector: {}\n    ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\"\n    # podMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.\n    podMonitorNamespaceSelector: {}\n    ## Example which selects PodMonitor in namespaces with label \"prometheus\" set to \"somelabel\"\n    # podMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the probes created\n    ##\n    probeSelectorNilUsesHelmValues: true\n\n    ## Probes to be selected for target discovery.\n    ## If {}, select all Probes\n    ##\n    probeSelector: {}\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\n    # probeSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.\n    probeNamespaceSelector: {}\n    ## Example which selects Probe in namespaces with label \"prometheus\" set to \"somelabel\"\n    # probeNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the scrapeConfigs created\n    ##\n    scrapeConfigSelectorNilUsesHelmValues: true\n\n    ## scrapeConfigs to be selected for target discovery.\n    ## If {}, select all scrapeConfigs\n    ##\n    scrapeConfigSelector: {}\n    ## Example which selects scrapeConfigs with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.\n    scrapeConfigNamespaceSelector: {}\n    ## Example which selects scrapeConfig in namespaces with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## Maximum size of metrics\n    ##\n    retentionSize: \"\"\n\n    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration\n    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\n    tsdb:\n      outOfOrderTimeWindow: 0s\n\n    ## Enable compression of the write-ahead log using Snappy.\n    ##\n    walCompression: true\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ##\n    replicas: 1\n\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\n    ## Sharding is done on the content of the `__address__` target meta-label.\n    ##\n    shards: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Log format for Prometheus be configured in\n    ##\n    logFormat: logfmt\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the prometheus instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n    ## additionalRemoteRead is appended to remoteRead\n    additionalRemoteRead: []\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n    ## additionalRemoteWrite is appended to remoteWrite\n    additionalRemoteWrite: []\n\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\n    remoteWriteDashboards: false\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    ## Using PersistentVolumeClaim\n    ##\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## Using tmpfs volume\n    ##\n    #  emptyDir:\n    #    medium: Memory\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ## AdditionalScrapeConfigs can be defined as a list or as a templated string.\n    ##\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     targetLabel: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     targetLabel: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n    #\n    ## If scrape config contains a repetitive section, you may want to use a template.\n    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones\n    # additionalScrapeConfigs: |\n    #  - job_name: \"node-exporter\"\n    #    gce_sd_configs:\n    #    {{range $zone := .Values.gcp_zones}}\n    #    - project: \"project1\"\n    #      zone: \"{{$zone}}\"\n    #      port: 9100\n    #    {{end}}\n    #    relabel_configs:\n    #    ...\n\n\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalScrapeConfigs\n    additionalScrapeConfigsSecret: {}\n      # enabled: false\n      # name:\n      # key:\n\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\n    additionalPrometheusSecretsAnnotations: {}\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertManagerConfigs\n    additionalAlertManagerConfigsSecret: {}\n      # name:\n      # key:\n      # optional: false\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertRelabelConfigs\n    additionalAlertRelabelConfigsSecret: {}\n      # name:\n      # key:\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n      # secretProviderClass:\n      #   provider: gcp\n      #   parameters:\n      #     secrets: |\n      #       - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\"\n      #         fileName: \"objstore.yaml\"\n      ## ObjectStorageConfig configures object storage in Thanos.\n      # objectStorageConfig:\n      #   # use existing secret, if configured, objectStorageConfig.secret will not be used\n      #   existingSecret: {}\n      #     # name: \"\"\n      #     # key: \"\"\n      #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,\n      #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set\n      #   # https://thanos.io/tip/thanos/storage.md/#s3\n      #   secret: {}\n      #     # type: S3\n      #     # config:\n      #     #   bucket: \"\"\n      #     #   endpoint: \"\"\n      #     #   region: \"\"\n      #     #   access_key: \"\"\n      #     #   secret_key: \"\"\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ## if using proxy extraContainer update targetPort with proxy container port\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\n    #   args:\n    #   - --upstream=http://127.0.0.1:9090\n    #   - --http-address=0.0.0.0:8081\n    #   - --metrics-address=0.0.0.0:8082\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   - containerPort: 8082\n    #     name: oauth-metrics\n    #     protocol: TCP\n    #   resources: {}\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## PortName to use for Prometheus.\n    ##\n    portName: \"http-web\"\n\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\n    ## on the file system of the Prometheus container e.g. bearer token files.\n    arbitraryFSAccessThroughSMs: false\n\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\n    ## or PodMonitor to true, this overrides honor_labels to false.\n    overrideHonorLabels: false\n\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\n    overrideHonorTimestamps: false\n\n    ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,\n    ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,\n    ## and servicemonitors will be installed in the default service namespace.\n    ## Defaults to false.\n    ignoreNamespaceSelectors: false\n\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\n    ## The label value will always be the namespace of the object that is being created.\n    ## Disabled by default\n    enforcedNamespaceLabel: \"\"\n\n    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\n    ## Deprecated, use `excludedFromEnforcement` instead\n    prometheusRulesExcludedFromEnforce: []\n\n    ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects\n    ## to be excluded from enforcing a namespace label of origin.\n    ## Works only if enforcedNamespaceLabel set to true.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference\n    excludedFromEnforcement: []\n\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\n    queryLogFile: false\n\n    # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.\n    # Set to 'false' to disable global sample_limit. or set to a number to override the default value.\n    sampleLimit: false\n\n    # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.\n    # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets\n    # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.\n    enforcedKeepDroppedTargets: 0\n\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\n    enforcedSampleLimit: false\n\n    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set\n    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall\n    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except\n    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.\n    enforcedTargetLimit: false\n\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelLimit: false\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelNameLengthLimit: false\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this\n    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus\n    ## versions 2.27.0 and newer.\n    enforcedLabelValueLengthLimit: false\n\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\n    ## in Prometheus so it may change in any upcoming release.\n    allowOverlappingBlocks: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n    # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.\n    # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.\n    hostNetwork: false\n\n    # HostAlias holds the mapping between IP and hostnames that will be injected\n    # as an entry in the pods hosts file.\n    hostAliases: []\n    #  - ip: 10.10.0.100\n    #    hostnames:\n    #      - a1.app.local\n    #      - b1.app.local\n\n    ## TracingConfig configures tracing in Prometheus.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig\n    tracingConfig: {}\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\n    ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)\n    additionalConfigString: \"\"\n\n    ## Defines the maximum time that the `prometheus` container's startup probe\n    ## will wait before being considered failed. The startup probe will return\n    ## success after the WAL replay is complete. If set, the value should be\n    ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15\n    ## minutes).\n    maximumStartupDurationSeconds: 0\n\n  additionalRulesForClusterRole: []\n  #  - apiGroups: [ \"\" ]\n  #    resources:\n  #      - nodes/proxy\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## labels to transfer from the kubernetes service to the target\n    ##\n    # targetLabels: []\n\n    ## labels to transfer from the kubernetes pods to the target\n    ##\n    # podTargetLabels: []\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  additionalPodMonitors: []\n  ## Name of the PodMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the pod endpoint name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for pods to which this PodMonitor applies\n    ##\n    # selector: {}\n\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\n    ##\n    # podTargetLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    # sampleLimit: 0\n\n    ## Namespaces from which pods are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected pods to be monitored\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint\n    ##\n    # podMetricsEndpoints: []\n\n## Configuration for thanosRuler\n## ref: https://thanos.io/tip/components/rule.md/\n##\nthanosRuler:\n\n  ## Deploy thanosRuler\n  ##\n  enabled: false\n\n  ## Annotations for ThanosRuler\n  ##\n  annotations: {}\n\n  ## Service account for ThanosRuler to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  ## Configure pod disruption budgets for ThanosRuler\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanosruler.domain.com\n\n    ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for ThanosRuler Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanosruler-general-tls\n    #   hosts:\n    #   - thanosruler.example.com\n\n  ## Configuration for ThanosRuler service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    ipDualStack:\n      enabled: false\n      ipFamilies: [\"IPv6\", \"IPv4\"]\n      ipFamilyPolicy: \"PreferDualStack\"\n\n    ## Port for ThanosRuler Service to listen on\n    ##\n    port: 10902\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 10902\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30905\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for ThanosRuler service\n    additionalPorts: []\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a ServiceMonitor for the ThanosRuler service\n  ##\n  serviceMonitor:\n    ## If true, create a serviceMonitor for thanosRuler\n    ##\n    selfMonitor: true\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional Endpoints\n    ##\n    additionalEndpoints: []\n    # - port: oauth-metrics\n    #   path: /metrics\n\n  ## Settings affecting thanosRulerpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec\n  ##\n  thanosRulerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.\n    ##\n    podMetadata: {}\n\n    ## Image of ThanosRuler\n    ##\n    image:\n      registry: quay.io\n      repository: thanos/thanos\n      tag: v0.35.1\n      sha: \"\"\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for ThanosRuler to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 24h\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## Storage is the definition of how storage will be used by the ThanosRuler instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n    ## AlertmanagerConfig define configuration for connecting to alertmanager.\n    ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.\n    alertmanagersConfig:\n      # use existing secret, if configured, alertmanagersConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n        # key: \"\"\n      # will render render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set\n      # https://thanos.io/tip/components/rule.md/#alertmanager\n      secret: {}\n        # alertmanagers:\n        # - api_version: v2\n        #   http_config:\n        #     basic_auth:\n        #       username: some_user\n        #       password: some_pass\n        #   static_configs:\n        #     - alertmanager.thanos.io\n        #   scheme: http\n        #   timeout: 10s\n\n    ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.\n    ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.\n    # alertmanagersUrl:\n\n    ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false\n    ##\n    externalPrefix:\n\n    ## If true, http://{{ template \"kube-prometheus-stack.thanosRuler.name\" . }}.{{ template \"kube-prometheus-stack.namespace\" . }}:{{ .Values.thanosRuler.service.port }}\n    ## will be used as value for externalPrefix\n    externalPrefixNilUsesHelmValues: true\n\n    ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## ObjectStorageConfig configures object storage in Thanos\n    objectStorageConfig:\n      # use existing secret, if configured, objectStorageConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n        # key: \"\"\n      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set\n      # https://thanos.io/tip/thanos/storage.md/#s3\n      secret: {}\n        # type: S3\n        # config:\n        #   bucket: \"\"\n        #   endpoint: \"\"\n        #   region: \"\"\n        #   access_key: \"\"\n        #   secret_key: \"\"\n\n    ## Labels by name to drop before sending to alertmanager\n    ## Maps to the --alert.label-drop flag of thanos ruler.\n    alertDropLabels: []\n\n    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.\n    ## Maps to the --query flag of thanos ruler.\n    queryEndpoints: []\n\n    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.\n    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.\n    queryConfig:\n      # use existing secret, if configured, queryConfig.secret will not be used\n      existingSecret: {}\n        # name: \"\"\n        # key: \"\"\n      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set\n      # https://thanos.io/tip/components/rule.md/#query-api\n      secret: {}\n        # - http_config:\n        #     basic_auth:\n        #       username: some_user\n        #       password: some_pass\n        #   static_configs:\n        #     - URL\n        #   scheme: http\n        #   timeout: 10s\n\n    ## Labels configure the external label pairs to ThanosRuler. A default replica\n    ## label `thanos_ruler_replica` will be always added as a label with the value\n    ## of the pod's name and it will be dropped in the alerts.\n    labels: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Allows setting additional arguments for the ThanosRuler container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler\n    ##\n    additionalArgs: []\n      # - name: remote-write.config\n      #   value: |-\n      #     \"remote_write\":\n      #     - \"name\": \"receiver-0\"\n      #       \"remote_timeout\": \"30s\"\n      #       \"url\": \"http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the thanosRuler instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: thanos-ruler\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the ThanosRuler UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.\n    ##\n    containers: []\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## PortName to use for ThanosRuler.\n    ##\n    portName: \"web\"\n\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\n    additionalConfig: {}\n\n    ## Additional configuration which is not covered by the properties above.\n    ## Useful, if you need advanced templating\n    additionalConfigString: \"\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.\n##\ncleanPrometheusOperatorObjectNames: false\n\n## Extra manifests to deploy as an array\nextraManifests: []\n  # - apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n  #     extra-data: \"value\"\n"],"verify":false,"version":"61.3.2","wait":true,"wait_for_jobs":false},"sensitive_attributes":[[{"type":"get_attr","value":"repository_password"}]],"private":"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==","dependencies":["kubernetes_namespace.prometheus_ns"]}]},{"mode":"managed","type":"kubernetes_ingress_v1","name":"loki-ingress","provider":"provider[\"registry.opentofu.org/hashicorp/kubernetes\"]","instances":[{"schema_version":0,"attributes":{"id":"loki-stack/loki-ingress","metadata":[{"annotations":{"kubernetes.io/ingress.class":"nginx"},"generate_name":"","generation":1,"labels":null,"name":"loki-ingress","namespace":"loki-stack","resource_version":"479138","uid":"d61fb72a-dc31-4270-814c-44f979205055"}],"spec":[{"default_backend":[],"ingress_class_name":"nginx","rule":[{"host":"","http":[{"path":[{"backend":[{"resource":[],"service":[{"name":"loki-name-grafana","port":[{"name":"","number":3000}]}]}],"path":"/loki","path_type":"ImplementationSpecific"}]}]}],"tls":[]}],"status":[{"load_balancer":[{"ingress":[]}]}],"timeouts":null,"wait_for_load_balancer":null},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDB9fQ==","dependencies":["kubernetes_namespace.loki_ns"]}]},{"mode":"managed","type":"kubernetes_namespace","name":"ingress-nginx_ns","provider":"provider[\"registry.opentofu.org/hashicorp/kubernetes\"]","instances":[{"schema_version":0,"attributes":{"id":"ingress-nginx","metadata":[{"annotations":{"name":"ingress-nginx"},"generate_name":"","generation":0,"labels":{"role":"ingress-nginx"},"name":"ingress-nginx","resource_version":"323169","uid":"87066016-330c-4506-8c36-5d27435c56fe"}],"timeouts":null,"wait_for_default_service_account":false},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="}]},{"mode":"managed","type":"kubernetes_namespace","name":"loki_ns","provider":"provider[\"registry.opentofu.org/hashicorp/kubernetes\"]","instances":[{"schema_version":0,"attributes":{"id":"loki-stack","metadata":[{"annotations":{"name":"loki-stack"},"generate_name":"","generation":0,"labels":{"role":"loki-stack"},"name":"loki-stack","resource_version":"479135","uid":"a605223d-984a-419f-bcfe-97753911a53d"}],"timeouts":null,"wait_for_default_service_account":false},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="}]},{"mode":"managed","type":"kubernetes_namespace","name":"prometheus_ns","provider":"provider[\"registry.opentofu.org/hashicorp/kubernetes\"]","instances":[{"schema_version":0,"attributes":{"id":"prometheus","metadata":[{"annotations":{"name":"prometheus"},"generate_name":"","generation":0,"labels":{"role":"prometheus"},"name":"prometheus","resource_version":"14436","uid":"9644c0de-27b6-492b-b683-5582934ac4b9"}],"timeouts":null,"wait_for_default_service_account":false},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="}]}],"check_results":null}
